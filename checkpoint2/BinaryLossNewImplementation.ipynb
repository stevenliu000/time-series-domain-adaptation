{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryLossNewImplementation(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(BinaryLossNewImplementation, self).__init__()\n",
    "        self.device = device    \n",
    "\n",
    "    def forward(self, fake_x_embedding, source_x_embedding, num_class, num_per_class, mask=None):\n",
    "        \"\"\"\n",
    "        fake_x_embedding:  [x1, x1, x1, ..., x2, x2, x2, ..., xn, xn, xn, ...]\n",
    "        source_x_embedding: [x1, x1, x1, ..., x2, x2, x2, ..., xn, xn, xn, ...]\n",
    "        (where n is num_class)\n",
    "        \n",
    "        for each class, data repeats num_per_class times.\n",
    "        \"\"\"\n",
    "        if type(mask) == torch.Tensor:\n",
    "            mask = mask.to(self.device)\n",
    "        elif type(mask) == numpy.ndarray:\n",
    "            mask = torch.LongTensor(mask).to(device)\n",
    "        else:\n",
    "            mask = torch.ones([num_per_class * num_class, num_per_class * num_class]).to(self.device)\n",
    "        assert mask.shape == torch.Size([num_per_class * num_class, num_per_class * num_class])\n",
    "        \n",
    "        labels = torch.zeros([num_per_class * num_class, num_per_class * num_class])\n",
    "        for i in range(num_class):\n",
    "            labels[i*num_per_class:(i+1)*num_per_class, i*num_per_class:(i+1)*num_per_class] = 1\n",
    "        labels = labels.to(self.device) # labels is a 1 block-diagnoal matrix\n",
    "        ones_l1_norm = torch.sum(labels) # number of pairs whose labels are 1\n",
    "        zeros_l1_norm = torch.sum(1-labels) # number of pairs whose labels are 0\n",
    "        \n",
    "        ones_weights = labels / ones_l1_norm\n",
    "        zeros_weights = (1 - labels) / zeros_l1_norm\n",
    "        weights = ones_weights + zeros_weights\n",
    "        weights = weights.to(self.device)\n",
    "        weights = weights * mask\n",
    "        \n",
    "        logits = torch.matmul(fake_x_embedding, source_x_embedding.transpose(0,1))\n",
    "        # weights already includes the normalization term\n",
    "        loss_mean = F.binary_cross_entropy_with_logits(logits, labels, weight=weights, reduction='sum')\n",
    "\n",
    "        return loss_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape == torch.Size([2,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
