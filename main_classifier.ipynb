{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesChunkDataset(data.Dataset):\n",
    "    def __init__(self, x, y, context):\n",
    "        super(TimeSeriesChunkDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.context = context\n",
    "        self.points_per_series = self.x.shape[1] - self.context + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0] * self.points_per_series\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_series = index // self.points_per_series\n",
    "        index_point = index % self.points_per_series\n",
    "        return_x = self.x[index_series, index_point:index_point+self.context, :]\n",
    "        return_y = np.argmax(self.y[index_series, :])\n",
    "        return return_x, return_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, layers_size, dim_out, dropout=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layers = []\n",
    "        self.activs = []\n",
    "        self.dropouts = []\n",
    "        self.layers_size = layers_size\n",
    "        \n",
    "        for i in range(len(layers_size)-1):\n",
    "            self.layers.append(nn.Linear(layers_size[i], layers_size[i+1]))\n",
    "            self.activs.append(nn.ReLU())\n",
    "            if i < len(layers_size)-2:\n",
    "                self.dropouts.append(nn.Dropouts(p=dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(layers_size[-1], dim_out))\n",
    "        self.nlayer = len(self.layers)\n",
    "        self.nactivs = len(self.activs)\n",
    "        self.ndropouts = len(self.dropouts)\n",
    "\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.nlayer):\n",
    "            out = self.layers[i](out)\n",
    "            if i < self.nactivs:\n",
    "                out = self.activs[i](out)\n",
    "            if i < self.ndropouts:\n",
    "                out = self.dropouts[i](out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    credit: from https://github.com/martinmamql/complex_da\n",
    "    '''\n",
    "    def __init__(self, feature_dim, d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Generator, self).__init__()\n",
    "        arguments = {'d_model':2, 'nhead':1, 'num_encoder_layers':3, 'num_decoder_layers':3, 'dim_feedforward':1024, 'dropout':0.1, 'activation':'gelu'}\n",
    "        arguments.update(kwargs)\n",
    "        self.transformer = nn.Transformer(**arguments)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceDomainClassifier():\n",
    "    def __init__(self, context, dim_out, **kwargs):\n",
    "        super(SourceDomainClassifier, self).__init__()\n",
    "        self.transformation = Generator(**kwargs)\n",
    "        self.classifier = Classifier(context*2, dim_out, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformation(x)\n",
    "        x = nn.Module.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, lr, n_epochs, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    train_acc_ = []\n",
    "    vali_acc_ = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        num_data = 0.0\n",
    "        num_batches = len(train_dataloader)\n",
    "        for batches, (x_batch, y_batch) in tqdm(enumerate(train_dataloader), total=num_batches):\n",
    "            model.train()\n",
    "            num_data += y_batch.shape[0]\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_batch)\n",
    "            loss = loss_fn(preds, y_batch.squeeze_())\n",
    "            acc = get_accuracy(preds, y_batch, normalize=False)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, data_path, task, batch_size, epochs, lr, context):\n",
    "        self.data_path = data_path\n",
    "        self.task = task\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.context = context\n",
    "        \n",
    "args = fake_args('./data_unzip/,', '3A', 10, 10, 1e-3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    args.task = \"processed_file_3Av2.pkl\" if args.task == \"3A\" else \"processed_file_3E.pkl\"\n",
    "    args.data_path = args.data_path + args.task\n",
    "    data = np.load(args.data_path, allow_pickle=True)\n",
    "    train_dataset = TimeSeriesChunkDataset(data['tr_data'], data['tr_data'], args.context)\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
