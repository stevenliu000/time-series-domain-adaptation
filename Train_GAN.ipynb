{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-714050ec4f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Logger'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesChunkDataset(data.Dataset):\n",
    "    def __init__(self, x, y, context):\n",
    "        super(TimeSeriesChunkDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.context = context\n",
    "        self.points_per_series = self.x.shape[1] - self.context + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0] * self.points_per_series\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_series = index // self.points_per_series\n",
    "        index_point = index % self.points_per_series\n",
    "        return_x = self.x[index_series, index_point:index_point+self.context, :]\n",
    "        return_y = np.argmax(self.y[index_series, :])\n",
    "        return return_x, return_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    credit: from https://github.com/martinmamql/complex_da\n",
    "    '''\n",
    "    def __init__(self, feature_dim, d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Generator, self).__init__()\n",
    "        self.transformer_layer_args = {'d_model':2, 'nhead':1, 'dim_feedforward':1024, 'dropout':0.1, 'activation':'gelu'}\n",
    "        self.transformer_args = {'num_layers':3, 'norm':None}\n",
    "        self.transformer_layer_args.update(kwargs['transformer_layer'])\n",
    "        self.transformer_args.update(kwargs['transformer'])\n",
    "        \n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(**self.transformer_layer_args)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, **self.transformer_args)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_out(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def target_out(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, trans_source_data, trans_target_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on source Data\n",
    "    prediction_source = discriminator_GAN(trans_source_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_source = loss(prediction_source, source_out(trans_source_data.size(0)))\n",
    "    error_source.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_target = discriminator_GAN(trans_target_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_target = loss(prediction_target, target_out(trans_target_data.size(0)))\n",
    "    error_target.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_source + error_target, prediction_source, prediction_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, target_data):\n",
    "    # 2. Train Generator\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator_GAN(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, source_out(prediction.size(0)))\n",
    "    error.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        \n",
    "args = fake_args(data_path='./data_unzip/', \n",
    "                 task='3A', \n",
    "                 batch_size=30,\n",
    "                 epochs=2,\n",
    "                 lr=1e-3,\n",
    "                 context=10)\n",
    "\n",
    "args.task = \"processed_file_3Av2.pkl\" if args.task == \"3A\" else \"processed_file_3E.pkl\"\n",
    "args.data_path = args.data_path + args.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    data_dict = np.load(args.data_path, allow_pickle=True)\n",
    "    \n",
    "    # TODO: to be commented\n",
    "    data_dict['tr_data'] = data_dict['tr_data'][:10]\n",
    "    data_dict['tr_data'] = data_dict['tr_data'][:10]\n",
    "    \n",
    "    \n",
    "    data_dict['te_data'] = data_dict['te_data'][:10]\n",
    "    data_dict['te_data'] = data_dict['te_data'][:10]\n",
    "    \n",
    "    # split train data and validation data for source data\n",
    "    np.random.seed(seed=0)\n",
    "    indices = np.random.permutation(data_dict['tr_data'].shape[0])\n",
    "    source_train_x = data_dict['tr_data'][indices[:int(indices.shape[0]*0.9)],:,:].astype(\"float32\")\n",
    "    source_train_y = data_dict['tr_lbl'][indices[:int(indices.shape[0]*0.9)],:].astype(\"float32\")\n",
    "    source_vali_x = data_dict['tr_data'][indices[int(indices.shape[0]*0.9):],:,:].astype(\"float32\")\n",
    "    source_vali_y = data_dict['tr_lbl'][indices[int(indices.shape[0]*0.9):],:].astype(\"float32\")\n",
    "    \n",
    "    source_train_dataset = TimeSeriesChunkDataset(source_train_x, source_train_y, args.context)\n",
    "    source_train_dataloader = data.DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    source_vali_dataset = TimeSeriesChunkDataset(source_vali_x, source_vali_y, args.context)\n",
    "    source_vali_dataloader = data.DataLoader(source_vali_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    # split train data and validation data for target data\n",
    "    np.random.seed(seed=0)\n",
    "    indices = np.random.permutation(data_dict['te_data'].shape[0])\n",
    "    target_train_x = data_dict['te_data'][indices[:int(indices.shape[0]*0.9)],:,:].astype(\"float32\")\n",
    "    target_train_y = data_dict['te_lbl'][indices[:int(indices.shape[0]*0.9)],:].astype(\"float32\")\n",
    "    target_vali_x = data_dict['te_data'][indices[int(indices.shape[0]*0.9):],:,:].astype(\"float32\")\n",
    "    target_vali_y = data_dict['te_lbl'][indices[int(indices.shape[0]*0.9):],:].astype(\"float32\")\n",
    "    \n",
    "    target_train_dataset = TimeSeriesChunkDataset(target_train_x, target_train_y, args.context)\n",
    "    target_train_dataloader = data.DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    target_vali_dataset = TimeSeriesChunkDataset(target_vali_x, target_vali_y, args.context)\n",
    "    target_vali_dataloader = data.DataLoader(target_vali_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    model_args = {\n",
    "        'classifier': {\n",
    "            'layers_size': [args.context*2, 100, 100], \n",
    "            'dim_out': 50 if args.task == '3A' else 65\n",
    "        },\n",
    "        'generator':{\n",
    "            'transformer_layer': {},\n",
    "            'transformer': {}\n",
    "        }, \n",
    "        'discriminator': {\n",
    "            'feature_dim': args.context*2, ## ?\n",
    "            'd_out': 50 if args.task == '3A' else 65\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"transformer_layer.self_attn.in_proj_weight\", \"transformer_layer.self_attn.in_proj_bias\", \"transformer_layer.self_attn.out_proj.weight\", \"transformer_layer.self_attn.out_proj.bias\", \"transformer_layer.linear1.weight\", \"transformer_layer.linear1.bias\", \"transformer_layer.linear2.weight\", \"transformer_layer.linear2.bias\", \"transformer_layer.norm1.weight\", \"transformer_layer.norm1.bias\", \"transformer_layer.norm2.weight\", \"transformer_layer.norm2.bias\", \"transformer.layers.0.self_attn.in_proj_weight\", \"transformer.layers.0.self_attn.in_proj_bias\", \"transformer.layers.0.self_attn.out_proj.weight\", \"transformer.layers.0.self_attn.out_proj.bias\", \"transformer.layers.0.linear1.weight\", \"transformer.layers.0.linear1.bias\", \"transformer.layers.0.linear2.weight\", \"transformer.layers.0.linear2.bias\", \"transformer.layers.0.norm1.weight\", \"transformer.layers.0.norm1.bias\", \"transformer.layers.0.norm2.weight\", \"transformer.layers.0.norm2.bias\", \"transformer.layers.1.self_attn.in_proj_weight\", \"transformer.layers.1.self_attn.in_proj_bias\", \"transformer.layers.1.self_attn.out_proj.weight\", \"transformer.layers.1.self_attn.out_proj.bias\", \"transformer.layers.1.linear1.weight\", \"transformer.layers.1.linear1.bias\", \"transformer.layers.1.linear2.weight\", \"transformer.layers.1.linear2.bias\", \"transformer.layers.1.norm1.weight\", \"transformer.layers.1.norm1.bias\", \"transformer.layers.1.norm2.weight\", \"transformer.layers.1.norm2.bias\", \"transformer.layers.2.self_attn.in_proj_weight\", \"transformer.layers.2.self_attn.in_proj_bias\", \"transformer.layers.2.self_attn.out_proj.weight\", \"transformer.layers.2.self_attn.out_proj.bias\", \"transformer.layers.2.linear1.weight\", \"transformer.layers.2.linear1.bias\", \"transformer.layers.2.linear2.weight\", \"transformer.layers.2.linear2.bias\", \"transformer.layers.2.norm1.weight\", \"transformer.layers.2.norm1.bias\", \"transformer.layers.2.norm2.weight\", \"transformer.layers.2.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.layers.in_proj_weight.linear0.bias\", \"transformer.layers.in_proj_bias.linear0.bias\", \"transformer.layers.out_proj.linear0.bias\", \"transformer.layers.weight.linear0.bias\", \"transformer.layers.bias.linear0.bias\", \"transformer.layers.weight.linear2.bias\", \"transformer.layers.bias.linear2.bias\", \"transformer.layers.0.linear0.bias\", \"transformer.layers.1.linear0.bias\", \"transformer.layers.2.linear0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fa93564df91e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msource_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_1.t7'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msource_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"transformer_layer.self_attn.in_proj_weight\", \"transformer_layer.self_attn.in_proj_bias\", \"transformer_layer.self_attn.out_proj.weight\", \"transformer_layer.self_attn.out_proj.bias\", \"transformer_layer.linear1.weight\", \"transformer_layer.linear1.bias\", \"transformer_layer.linear2.weight\", \"transformer_layer.linear2.bias\", \"transformer_layer.norm1.weight\", \"transformer_layer.norm1.bias\", \"transformer_layer.norm2.weight\", \"transformer_layer.norm2.bias\", \"transformer.layers.0.self_attn.in_proj_weight\", \"transformer.layers.0.self_attn.in_proj_bias\", \"transformer.layers.0.self_attn.out_proj.weight\", \"transformer.layers.0.self_attn.out_proj.bias\", \"transformer.layers.0.linear1.weight\", \"transformer.layers.0.linear1.bias\", \"transformer.layers.0.linear2.weight\", \"transformer.layers.0.linear2.bias\", \"transformer.layers.0.norm1.weight\", \"transformer.layers.0.norm1.bias\", \"transformer.layers.0.norm2.weight\", \"transformer.layers.0.norm2.bias\", \"transformer.layers.1.self_attn.in_proj_weight\", \"transformer.layers.1.self_attn.in_proj_bias\", \"transformer.layers.1.self_attn.out_proj.weight\", \"transformer.layers.1.self_attn.out_proj.bias\", \"transformer.layers.1.linear1.weight\", \"transformer.layers.1.linear1.bias\", \"transformer.layers.1.linear2.weight\", \"transformer.layers.1.linear2.bias\", \"transformer.layers.1.norm1.weight\", \"transformer.layers.1.norm1.bias\", \"transformer.layers.1.norm2.weight\", \"transformer.layers.1.norm2.bias\", \"transformer.layers.2.self_attn.in_proj_weight\", \"transformer.layers.2.self_attn.in_proj_bias\", \"transformer.layers.2.self_attn.out_proj.weight\", \"transformer.layers.2.self_attn.out_proj.bias\", \"transformer.layers.2.linear1.weight\", \"transformer.layers.2.linear1.bias\", \"transformer.layers.2.linear2.weight\", \"transformer.layers.2.linear2.bias\", \"transformer.layers.2.norm1.weight\", \"transformer.layers.2.norm1.bias\", \"transformer.layers.2.norm2.weight\", \"transformer.layers.2.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.layers.in_proj_weight.linear0.bias\", \"transformer.layers.in_proj_bias.linear0.bias\", \"transformer.layers.out_proj.linear0.bias\", \"transformer.layers.weight.linear0.bias\", \"transformer.layers.bias.linear0.bias\", \"transformer.layers.weight.linear2.bias\", \"transformer.layers.bias.linear2.bias\", \"transformer.layers.0.linear0.bias\", \"transformer.layers.1.linear0.bias\", \"transformer.layers.2.linear0.bias\". "
     ]
    }
   ],
   "source": [
    "# Discriminator and Generator\n",
    "def map_keys(trained):\n",
    "    # modify for new trained data\n",
    "    net, layer  = trained.split(\".\")[1], trained.split(\".\")[4]\n",
    "    return '.'.join(['transformer', 'layers', layer, 'linear' + str(net), 'bias'])\n",
    "\n",
    "def transform_dict(dict_):\n",
    "    return {map_keys(i): dict_[i] for i in dict_}\n",
    "\n",
    "\n",
    "discriminator_GAN = Discriminator(**(model_args[\"discriminator\"]))\n",
    "generator_GAN = Generator(**(model_args[\"generator\"]))\n",
    "source_transformer = Generator(**(model_args[\"generator\"]))\n",
    "PATH = 'model_1.t7'\n",
    "source_transformer.load_state_dict(transform_dict(torch.load(PATH)))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    discriminator_GAN.cuda()\n",
    "    generator_GAN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.transformer_layer.self_attn.in_proj_weight',\n",
       "              tensor([[-0.6529,  0.8293],\n",
       "                      [-0.7535,  0.7948],\n",
       "                      [ 0.2679, -0.4639],\n",
       "                      [ 0.8128,  0.4654],\n",
       "                      [-0.2494,  0.4241],\n",
       "                      [ 0.3518, -0.2487]])),\n",
       "             ('net.0.transformer_layer.self_attn.in_proj_bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0.])),\n",
       "             ('net.0.transformer_layer.self_attn.out_proj.weight',\n",
       "              tensor([[-0.0883,  0.1629],\n",
       "                      [-0.1901,  0.3037]])),\n",
       "             ('net.0.transformer_layer.self_attn.out_proj.bias',\n",
       "              tensor([0., 0.])),\n",
       "             ('net.0.transformer_layer.linear1.weight',\n",
       "              tensor([[ 0.0228, -0.0206],\n",
       "                      [-0.0679, -0.0408],\n",
       "                      [-0.0758,  0.0558],\n",
       "                      ...,\n",
       "                      [ 0.0497, -0.0555],\n",
       "                      [ 0.0149,  0.0028],\n",
       "                      [-0.0213, -0.0441]])),\n",
       "             ('net.0.transformer_layer.linear1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.0.transformer_layer.linear2.weight',\n",
       "              tensor([[-9.6733e-03,  1.6995e-02, -3.5639e-02,  ..., -8.1764e-03,\n",
       "                       -4.5577e-02,  3.4287e-02],\n",
       "                      [-7.6666e-05, -2.9365e-03,  5.4786e-02,  ...,  7.0582e-02,\n",
       "                        6.9640e-02,  4.6107e-02]])),\n",
       "             ('net.0.transformer_layer.linear2.bias', tensor([0., 0.])),\n",
       "             ('net.0.transformer_layer.norm1.weight', tensor([1., 1.])),\n",
       "             ('net.0.transformer_layer.norm1.bias', tensor([0., 0.])),\n",
       "             ('net.0.transformer_layer.norm2.weight', tensor([1., 1.])),\n",
       "             ('net.0.transformer_layer.norm2.bias', tensor([0., 0.])),\n",
       "             ('net.0.transformer.layers.0.self_attn.in_proj_weight',\n",
       "              tensor([[-0.6529,  0.8293],\n",
       "                      [-0.7535,  0.7948],\n",
       "                      [ 0.2679, -0.4639],\n",
       "                      [ 0.8128,  0.4654],\n",
       "                      [-0.2494,  0.4241],\n",
       "                      [ 0.3518, -0.2487]])),\n",
       "             ('net.0.transformer.layers.0.self_attn.in_proj_bias',\n",
       "              tensor([ 9.6978e-13,  1.6961e-13, -2.8122e-21, -1.0418e-21, -1.2671e-10,\n",
       "                       2.3163e-11])),\n",
       "             ('net.0.transformer.layers.0.self_attn.out_proj.weight',\n",
       "              tensor([[ 1.0978, -0.0961],\n",
       "                      [-1.0169,  0.2904]])),\n",
       "             ('net.0.transformer.layers.0.self_attn.out_proj.bias',\n",
       "              tensor([-5.5342e-11,  5.5362e-11])),\n",
       "             ('net.0.transformer.layers.0.linear1.weight',\n",
       "              tensor([[-0.0405, -0.0327],\n",
       "                      [ 0.0287,  0.0481],\n",
       "                      [ 0.0020, -0.0718],\n",
       "                      ...,\n",
       "                      [-0.0680,  0.0223],\n",
       "                      [ 0.0355,  0.0568],\n",
       "                      [ 0.0057, -0.0113]])),\n",
       "             ('net.0.transformer.layers.0.linear1.bias',\n",
       "              tensor([-5.8312e-15, -4.5750e-15, -5.8694e-15,  ..., -8.1236e-17,\n",
       "                       5.9384e-15, -1.8197e-16])),\n",
       "             ('net.0.transformer.layers.0.linear2.weight',\n",
       "              tensor([[ 0.0575,  0.0453,  0.0579,  ..., -0.0114, -0.0585, -0.0219],\n",
       "                      [ 0.0653, -0.0050,  0.0209,  ...,  0.0407, -0.0268,  0.0424]])),\n",
       "             ('net.0.transformer.layers.0.linear2.bias',\n",
       "              tensor([-1.8229e-13, -1.7163e-16])),\n",
       "             ('net.0.transformer.layers.0.norm1.weight', tensor([1., 1.])),\n",
       "             ('net.0.transformer.layers.0.norm1.bias',\n",
       "              tensor([-1.6132e-13,  1.5375e-13])),\n",
       "             ('net.0.transformer.layers.0.norm2.weight', tensor([1., 1.])),\n",
       "             ('net.0.transformer.layers.0.norm2.bias',\n",
       "              tensor([-8.4441e-16,  6.8247e-16])),\n",
       "             ('net.0.transformer.layers.1.self_attn.in_proj_weight',\n",
       "              tensor([[-0.6529,  0.8293],\n",
       "                      [-0.7535,  0.7948],\n",
       "                      [ 0.2679, -0.4639],\n",
       "                      [ 0.8128,  0.4654],\n",
       "                      [-0.2494,  0.4241],\n",
       "                      [ 0.3518, -0.2487]])),\n",
       "             ('net.0.transformer.layers.1.self_attn.in_proj_bias',\n",
       "              tensor([-3.4991e-17, -1.6613e-17, -6.3810e-23, -5.7197e-23, -3.8632e-16,\n",
       "                      -8.6037e-16])),\n",
       "             ('net.0.transformer.layers.1.self_attn.out_proj.weight',\n",
       "              tensor([[ 1.1272, -0.1183],\n",
       "                      [ 0.1586, -1.0631]])),\n",
       "             ('net.0.transformer.layers.1.self_attn.out_proj.bias',\n",
       "              tensor([-3.6034e-16,  6.6284e-16])),\n",
       "             ('net.0.transformer.layers.1.linear1.weight',\n",
       "              tensor([[-0.0226,  0.0422],\n",
       "                      [ 0.0581,  0.0135],\n",
       "                      [-0.0666,  0.0676],\n",
       "                      ...,\n",
       "                      [ 0.0593, -0.0114],\n",
       "                      [ 0.0684, -0.0371],\n",
       "                      [-0.0676, -0.0325]])),\n",
       "             ('net.0.transformer.layers.1.linear1.bias',\n",
       "              tensor([ 5.7632e-12,  1.8279e-12,  7.1077e-12,  ...,  1.1827e-12,\n",
       "                      -5.6301e-12, -1.7648e-13])),\n",
       "             ('net.0.transformer.layers.1.linear2.weight',\n",
       "              tensor([[-0.0604, -0.0189, -0.0619,  ..., -0.0464, -0.0569, -0.0405],\n",
       "                      [ 0.0367, -0.0627,  0.0157,  ..., -0.0593,  0.0350, -0.0456]])),\n",
       "             ('net.0.transformer.layers.1.linear2.bias',\n",
       "              tensor([-1.4540e-12,  2.9232e-13])),\n",
       "             ('net.0.transformer.layers.1.norm1.weight', tensor([1., 1.])),\n",
       "             ('net.0.transformer.layers.1.norm1.bias',\n",
       "              tensor([ 7.3589e-12, -4.0015e-12])),\n",
       "             ('net.0.transformer.layers.1.norm2.weight',\n",
       "              tensor([1.0001, 1.0001])),\n",
       "             ('net.0.transformer.layers.1.norm2.bias',\n",
       "              tensor([ 6.2918e-07, -8.9602e-07])),\n",
       "             ('net.0.transformer.layers.2.self_attn.in_proj_weight',\n",
       "              tensor([[-0.6529,  0.8293],\n",
       "                      [-0.7534,  0.7947],\n",
       "                      [ 0.2679, -0.4638],\n",
       "                      [ 0.8128,  0.4654],\n",
       "                      [-0.2493,  0.4240],\n",
       "                      [ 0.3518, -0.2487]])),\n",
       "             ('net.0.transformer.layers.2.self_attn.in_proj_bias',\n",
       "              tensor([ 1.0903e-07,  5.1777e-08,  3.5332e-14,  2.6746e-13, -1.5260e-06,\n",
       "                      -3.9927e-07])),\n",
       "             ('net.0.transformer.layers.2.self_attn.out_proj.weight',\n",
       "              tensor([[-0.3820, -0.3552],\n",
       "                      [ 0.4653, -0.0490]])),\n",
       "             ('net.0.transformer.layers.2.self_attn.out_proj.bias',\n",
       "              tensor([ 6.7979e-07, -1.7541e-06])),\n",
       "             ('net.0.transformer.layers.2.linear1.weight',\n",
       "              tensor([[-0.1080,  0.0636],\n",
       "                      [ 0.0374, -0.0726],\n",
       "                      [-0.0630,  0.0528],\n",
       "                      ...,\n",
       "                      [-0.0161, -0.0440],\n",
       "                      [ 0.0013,  0.0224],\n",
       "                      [-0.0476, -0.0354]])),\n",
       "             ('net.0.transformer.layers.2.linear1.bias',\n",
       "              tensor([ 0.0103,  0.0120,  0.0090,  ...,  0.0008, -0.0015, -0.0004])),\n",
       "             ('net.0.transformer.layers.2.linear2.weight',\n",
       "              tensor([[-0.0987,  0.0896, -0.0537,  ...,  0.0009,  0.0483, -0.0731],\n",
       "                      [-0.0165, -0.1121,  0.0600,  ..., -0.0456,  0.0320, -0.0645]])),\n",
       "             ('net.0.transformer.layers.2.linear2.bias',\n",
       "              tensor([2.9458e-03, 1.0905e-05])),\n",
       "             ('net.0.transformer.layers.2.norm1.weight',\n",
       "              tensor([1.0602, 1.0614])),\n",
       "             ('net.0.transformer.layers.2.norm1.bias',\n",
       "              tensor([-0.0080,  0.0088])),\n",
       "             ('net.0.transformer.layers.2.norm2.weight',\n",
       "              tensor([1.1246, 1.1249])),\n",
       "             ('net.0.transformer.layers.2.norm2.bias',\n",
       "              tensor([-0.0037,  0.0053])),\n",
       "             ('net.2.layers.0.weight',\n",
       "              tensor([[ 0.1201, -0.0948,  0.0677,  ...,  0.1312, -0.1156, -0.1118],\n",
       "                      [ 0.1648,  0.2203,  0.0693,  ..., -0.2628,  0.2153,  0.0818],\n",
       "                      [-0.0764,  0.2591, -0.2097,  ...,  0.2000, -0.1976,  0.1020],\n",
       "                      ...,\n",
       "                      [ 0.0551, -0.0680,  0.1024,  ..., -0.1164,  0.1480, -0.1862],\n",
       "                      [-0.1898, -0.1934,  0.1171,  ..., -0.2237, -0.1724, -0.0337],\n",
       "                      [-0.0776, -0.0844,  0.1208,  ...,  0.0223, -0.0758,  0.0371]])),\n",
       "             ('net.2.layers.0.bias',\n",
       "              tensor([-0.0088,  0.0055,  0.0588, -0.0149,  0.0163, -0.0696,  0.0214,  0.0390,\n",
       "                      -0.0212, -0.0614,  0.0039, -0.0089, -0.0059, -0.0248, -0.0178, -0.0506,\n",
       "                      -0.0511,  0.0060,  0.0389,  0.0174, -0.0155,  0.0280, -0.0511,  0.0296,\n",
       "                      -0.0610, -0.0569,  0.0479,  0.0140, -0.0097, -0.0488,  0.0374,  0.0080,\n",
       "                       0.0559,  0.0337,  0.0061,  0.0120, -0.0403, -0.0114,  0.0669, -0.0254,\n",
       "                       0.0077, -0.0475, -0.0003,  0.0199, -0.0336,  0.0195,  0.0032, -0.0213,\n",
       "                       0.0125, -0.0215, -0.0177,  0.0225, -0.0518,  0.0244,  0.0264,  0.0140,\n",
       "                       0.0300, -0.0184,  0.0155, -0.0374,  0.0519, -0.0238,  0.0151,  0.0096,\n",
       "                      -0.0208, -0.0162, -0.0812,  0.0202,  0.0040, -0.0134,  0.0055,  0.0222,\n",
       "                      -0.0004,  0.0407, -0.0805,  0.0297,  0.0140, -0.0052, -0.0824, -0.0038,\n",
       "                       0.0349,  0.0111,  0.0065, -0.0073,  0.0107, -0.0452,  0.0278, -0.0128,\n",
       "                      -0.0956, -0.0417, -0.0278, -0.0023,  0.0204, -0.0080,  0.0096, -0.0022,\n",
       "                       0.0083,  0.0029, -0.0133, -0.0169])),\n",
       "             ('net.2.layers.1.weight',\n",
       "              tensor([[ 0.1830,  0.1775,  0.1704,  ...,  0.1197, -0.0868,  0.0680],\n",
       "                      [ 0.1295,  0.0790,  0.0793,  ...,  0.0640,  0.1670,  0.2145],\n",
       "                      [ 0.1261,  0.0699, -0.0629,  ..., -0.0205, -0.0875, -0.0213],\n",
       "                      ...,\n",
       "                      [-0.1399, -0.0031, -0.0962,  ...,  0.0362, -0.1835, -0.0832],\n",
       "                      [-0.1496, -0.0417,  0.1588,  ..., -0.1021,  0.1099, -0.0676],\n",
       "                      [ 0.0019,  0.0084, -0.0446,  ...,  0.1368,  0.0048,  0.1239]])),\n",
       "             ('net.2.layers.1.bias',\n",
       "              tensor([ 0.0464,  0.0590,  0.0119,  0.0539,  0.0486, -0.0352, -0.0336,  0.0608,\n",
       "                       0.0489,  0.0550, -0.0325, -0.0176,  0.0539,  0.0192,  0.0686,  0.0309,\n",
       "                       0.0463,  0.0612,  0.0757,  0.0283,  0.0927,  0.0063, -0.0312,  0.0364,\n",
       "                       0.0594,  0.0012,  0.0196,  0.0598,  0.0600, -0.0157,  0.0220, -0.0090,\n",
       "                      -0.0062,  0.0468, -0.0323,  0.0420,  0.0555,  0.0840,  0.0243,  0.0383,\n",
       "                      -0.0258, -0.0035, -0.0332,  0.0507, -0.0019, -0.0295,  0.0011, -0.0293,\n",
       "                       0.0794,  0.0735,  0.0334,  0.0648, -0.0452, -0.0436,  0.0417,  0.0559,\n",
       "                      -0.0070,  0.0738,  0.0849,  0.0371, -0.0294,  0.0375,  0.0573,  0.0473,\n",
       "                       0.0377,  0.0061,  0.0025,  0.0586,  0.0194,  0.0404, -0.0079,  0.0149,\n",
       "                      -0.0510,  0.0213,  0.0557,  0.0227,  0.0654, -0.0266,  0.0133,  0.0681,\n",
       "                      -0.0179,  0.0400,  0.0634, -0.0355,  0.0451, -0.0145, -0.0180, -0.0265,\n",
       "                       0.0545, -0.0182,  0.0269, -0.0243,  0.0397,  0.0331,  0.0378, -0.0225,\n",
       "                       0.0335, -0.0373, -0.0382, -0.0414])),\n",
       "             ('net.2.layers.2.weight',\n",
       "              tensor([[ 0.0769, -0.2365, -0.0071,  ...,  0.0598, -0.1577,  0.1325],\n",
       "                      [ 0.1653,  0.1096, -0.0925,  ..., -0.1562, -0.0175, -0.1097],\n",
       "                      [ 0.0573,  0.0700, -0.2369,  ...,  0.0216,  0.1353, -0.0444],\n",
       "                      ...,\n",
       "                      [ 0.0683, -0.1432, -0.0586,  ..., -0.0256,  0.0638, -0.1046],\n",
       "                      [-0.2635,  0.0160, -0.0208,  ..., -0.2205,  0.0716,  0.1021],\n",
       "                      [-0.0035,  0.0202, -0.1894,  ..., -0.0357, -0.1694,  0.0050]])),\n",
       "             ('net.2.layers.2.bias',\n",
       "              tensor([-0.0435,  0.0631, -0.0502, -0.0494, -0.0419, -0.0506, -0.0460, -0.0458,\n",
       "                      -0.0472, -0.0481, -0.0509,  0.0458, -0.0487, -0.0483, -0.0535, -0.0491,\n",
       "                       0.0102, -0.0531, -0.0462, -0.0486,  0.0390,  0.0345, -0.0415, -0.0518,\n",
       "                      -0.0500,  0.0172, -0.0437, -0.0472, -0.0496, -0.0485, -0.0502, -0.0486,\n",
       "                      -0.0462, -0.0479, -0.0469, -0.0514, -0.0541, -0.0515, -0.0477, -0.0415,\n",
       "                      -0.0554, -0.0475, -0.0452,  0.0353,  0.0512, -0.0524, -0.0524, -0.0540,\n",
       "                      -0.0483, -0.0503, -0.0462, -0.0451, -0.0529, -0.0446, -0.0547, -0.0486,\n",
       "                      -0.0520, -0.0502, -0.0488, -0.0452, -0.0475, -0.0534, -0.0517, -0.0546,\n",
       "                      -0.0531]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'model_1.t7'\n",
    "torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "d_optimizer_GAN = optim.Adam(discriminator_GAN.parameters(), lr=0.0002)\n",
    "g_optimizer_GAN = optim.Adam(generator_GAN.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Number of steps to apply to the discriminator\n",
    "d_steps = 1  # In Goodfellow et. al 2014 this variable is assigned to 1\n",
    "# Number of epochs\n",
    "num_epochs = 2\n",
    "num_batches = len(source_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-936684251101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Time-Series-GAN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Time-Series-Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Logger' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #for n_batch, (source_batch,_) in enumerate(source_train_dataloader):\n",
    "    for batches, (x_batch, y_batch) in tqdm(enumerate(train_dataloader), total=num_batches):    \n",
    "        \n",
    "\n",
    "        # 1. Train Discriminator\n",
    "        \n",
    "        trans_source_data = x_batch ### get from the previous transformer\n",
    "        \n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available(): trans_source_data = trans_source_data.cuda()\n",
    "        # Generate fake data\n",
    "        trans_target_data = generator_GAN(x_batch).detach() ## ? from generator get trans_target_data\n",
    "        # Train D\n",
    "        d_error, d_pred_source, d_pred_target = train_discriminator(d_optimizer_GAN,\n",
    "                                                                trans_source_data, trans_target_data)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        # Generate fake data\n",
    "        # target_data = generator_GAN(??) # get data from generator transformer\n",
    "        \n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer_GAN, trans_target_data)\n",
    "        # Log error\n",
    "        print(d_error, g_error, epoch, n_batch, num_batches)\n",
    "\n",
    "        # Display Progress\n",
    "        if (n_batch) % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            # test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
    "            # logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            #logger.display_status(\n",
    "            #    epoch, num_epochs, n_batch, num_batches,\n",
    "            #    d_error, g_error, d_pred_source, d_pred_target\n",
    "            #)\n",
    "        # Model Checkpoints\n",
    "        logger.save_models(generator_GAN, discriminator_GAN, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
