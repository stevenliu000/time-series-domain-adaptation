{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-714050ec4f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Logger'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from utils import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesChunkDataset(data.Dataset):\n",
    "    def __init__(self, x, y, context):\n",
    "        super(TimeSeriesChunkDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.context = context\n",
    "        self.points_per_series = self.x.shape[1] - self.context + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0] * self.points_per_series\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_series = index // self.points_per_series\n",
    "        index_point = index % self.points_per_series\n",
    "        return_x = self.x[index_series, index_point:index_point+self.context, :]\n",
    "        return_y = np.argmax(self.y[index_series, :])\n",
    "        return return_x, return_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    credit: from https://github.com/martinmamql/complex_da\n",
    "    '''\n",
    "    def __init__(self, , d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Generator, self).__init__()\n",
    "        self.transformer_layer_args = {'d_model':2, 'nhead':1, 'dim_feedforward':1024, 'dropout':0.1, 'activation':'gelu'}\n",
    "        self.transformer_args = {'num_layers':3, 'norm':None}\n",
    "        self.transformer_layer_args.update(kwargs['transformer_layer'])\n",
    "        self.transformer_args.update(kwargs['transformer'])\n",
    "        \n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(**self.transformer_layer_args)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, **self.transformer_args)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_out(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def target_out(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9a961e07461b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, trans_source_data, trans_target_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on source Data\n",
    "    prediction_source = discriminator_GAN(trans_source_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_source = loss(prediction_source, source_out(trans_source_data.size(0)))\n",
    "    error_source.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_target = discriminator_GAN(trans_target_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_target = loss(prediction_target, target_out(trans_target_data.size(0)))\n",
    "    error_target.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_source + error_target, prediction_source, prediction_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, target_data):\n",
    "    # 2. Train Generator\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator_GAN(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, source_out(prediction.size(0)))\n",
    "    error.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        \n",
    "args = fake_args(data_path='./data_unzip/', \n",
    "                 task='3A', \n",
    "                 batch_size=30,\n",
    "                 epochs=2,\n",
    "                 lr=1e-3,\n",
    "                 context=10)\n",
    "\n",
    "args.task = \"processed_file_3Av2.pkl\" if args.task == \"3A\" else \"processed_file_3E.pkl\"\n",
    "args.data_path = args.data_path + args.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    data_dict = np.load(args.data_path, allow_pickle=True)\n",
    "    \n",
    "    # TODO: to be commented\n",
    "    data_dict['tr_data'] = data_dict['tr_data'][:10]\n",
    "    data_dict['tr_data'] = data_dict['tr_data'][:10]\n",
    "    \n",
    "    \n",
    "    data_dict['te_data'] = data_dict['te_data'][:10]\n",
    "    data_dict['te_data'] = data_dict['te_data'][:10]\n",
    "    \n",
    "    # split train data and validation data for source data\n",
    "    np.random.seed(seed=0)\n",
    "    indices = np.random.permutation(data_dict['tr_data'].shape[0])\n",
    "    source_train_x = data_dict['tr_data'][indices[:int(indices.shape[0]*0.9)],:,:].astype(\"float32\")\n",
    "    source_train_y = data_dict['tr_lbl'][indices[:int(indices.shape[0]*0.9)],:].astype(\"float32\")\n",
    "    source_vali_x = data_dict['tr_data'][indices[int(indices.shape[0]*0.9):],:,:].astype(\"float32\")\n",
    "    source_vali_y = data_dict['tr_lbl'][indices[int(indices.shape[0]*0.9):],:].astype(\"float32\")\n",
    "    \n",
    "    source_train_dataset = TimeSeriesChunkDataset(source_train_x, source_train_y, args.context)\n",
    "    source_train_dataloader = data.DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    source_vali_dataset = TimeSeriesChunkDataset(source_vali_x, source_vali_y, args.context)\n",
    "    source_vali_dataloader = data.DataLoader(source_vali_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    # split train data and validation data for target data\n",
    "    np.random.seed(seed=0)\n",
    "    indices = np.random.permutation(data_dict['te_data'].shape[0])\n",
    "    target_train_x = data_dict['te_data'][indices[:int(indices.shape[0]*0.9)],:,:].astype(\"float32\")\n",
    "    target_train_y = data_dict['te_lbl'][indices[:int(indices.shape[0]*0.9)],:].astype(\"float32\")\n",
    "    target_vali_x = data_dict['te_data'][indices[int(indices.shape[0]*0.9):],:,:].astype(\"float32\")\n",
    "    target_vali_y = data_dict['te_lbl'][indices[int(indices.shape[0]*0.9):],:].astype(\"float32\")\n",
    "    \n",
    "    target_train_dataset = TimeSeriesChunkDataset(target_train_x, target_train_y, args.context)\n",
    "    target_train_dataloader = data.DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    target_vali_dataset = TimeSeriesChunkDataset(target_vali_x, target_vali_y, args.context)\n",
    "    target_vali_dataloader = data.DataLoader(target_vali_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    model_args = {\n",
    "        'classifier': {\n",
    "            'layers_size': [args.context*2, 100, 100], \n",
    "            'dim_out': 50 if args.task == '3A' else 65\n",
    "        },\n",
    "        'generator':{\n",
    "            'transformer_layer': {},\n",
    "            'transformer': {}\n",
    "        }, \n",
    "        'discriminator': {\n",
    "            'feature_dim': args.context*2, ## ?\n",
    "            'd_out': 50 if args.task == '3A' else 65\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator and Generator\n",
    "discriminator_GAN = Discriminator(**(model_args[\"discriminator\"]))\n",
    "generator_GAN = Generator(**(model_args[\"generator\"]))\n",
    "if torch.cuda.is_available():\n",
    "    discriminator_GAN.cuda()\n",
    "    generator_GAN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "d_optimizer_GAN = optim.Adam(discriminator_GAN.parameters(), lr=0.0002)\n",
    "g_optimizer_GAN = optim.Adam(generator_GAN.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Number of steps to apply to the discriminator\n",
    "d_steps = 1  # In Goodfellow et. al 2014 this variable is assigned to 1\n",
    "# Number of epochs\n",
    "num_epochs = 2\n",
    "num_batches = len(source_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-936684251101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Time-Series-GAN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Time-Series-Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Logger' is not defined"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name='Time-Series-GAN', data_name='Time-Series-Data')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #for n_batch, (source_batch,_) in enumerate(source_train_dataloader):\n",
    "    for batches, (x_batch, y_batch) in tqdm(enumerate(train_dataloader), total=num_batches):    \n",
    "        \n",
    "\n",
    "        # 1. Train Discriminator\n",
    "        trans_source_data = Variable(x_batch) ### get from the previous transformer\n",
    "        \n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available(): trans_source_data = trans_source_data.cuda()\n",
    "        # Generate fake data\n",
    "        trans_target_data = generator_GAN(x_batch).detach() ## ? from generator get trans_target_data\n",
    "        # Train D\n",
    "        d_error, d_pred_source, d_pred_target = train_discriminator(d_optimizer_GAN,\n",
    "                                                                trans_source_data, trans_target_data)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        # Generate fake data\n",
    "        # target_data = generator_GAN(??) # get data from generator transformer\n",
    "        \n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer_GAN, trans_target_data)\n",
    "        # Log error\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "\n",
    "        # Display Progress\n",
    "        if (n_batch) % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            # test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
    "            # logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_source, d_pred_target\n",
    "            )\n",
    "        # Model Checkpoints\n",
    "        logger.save_models(generator_GAN, discriminator_GAN, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
