{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "method used here is based on the following paper\n",
    "\n",
    "Germain Forestier et al. \"Generating synthetic time series to augment sparse datasets\". ICDM 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "sys.path.insert(0, os.path.join(parent_dir,'spring-break'))\n",
    "sys.path.insert(0, os.path.join(parent_dir,'Linear Classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\tslearn\\bases.py:14: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "from FNNLinear import FNNLinear\n",
    "from FNNSeparated import FNNSeparated\n",
    "from GAN import Generator, Discriminator\n",
    "from data_utils import *\n",
    "import argparse\n",
    "import logging\n",
    "import logging.handlers\n",
    "import pickle\n",
    "from centerloss import CenterLoss\n",
    "from DataSetLoader import JoinDataset, SingleDataset\n",
    "from torch.autograd import Variable\n",
    "from binaryloss import BinaryLoss\n",
    "\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier, KNeighborsTimeSeries\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed, parallel_backend, Memory\n",
    "import time\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--task TASK]\n",
      "                             [--target_lbl_percentage TARGET_LBL_PERCENTAGE]\n",
      "                             [--source_lbl_percentage SOURCE_LBL_PERCENTAGE]\n",
      "                             [--num_class NUM_CLASS]\n",
      "                             [--class_split CLASS_SPLIT]\n",
      "                             [--subset_count SUBSET_COUNT]\n",
      "                             [--duplicate_time DUPLICATE_TIME]\n",
      "                             [--save_path SAVE_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Tianqin Li\\AppData\\Roaming\\jupyter\\runtime\\kernel-df3df4bb-a043-4776-8cd1-f77b7388ddad.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, default=\"3E\", help=\"task type 3E or 3Av2\")\n",
    "parser.add_argument('--target_lbl_percentage', type=float, default=0.7, help='percentage of which target data has label')\n",
    "parser.add_argument('--source_lbl_percentage', type=float, default=0.7, help='percentage of which source data has label')\n",
    "\n",
    "parser.add_argument(\"--num_class\", type=int, default=\"65\", help=\"num class\")\n",
    "parser.add_argument(\"--class_split\", type=str, default=\"0-4\", help=\"class generated\")\n",
    "parser.add_argument(\"--subset_count\", type=int, default=\"20\", help=\"select number of subset from each class\")\n",
    "parser.add_argument(\"--duplicate_time\", type=int, default=\"1\", help=\"numer of duplication\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"../train_related/asd\", help=\"save path\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local only\n",
    "# class local_args:\n",
    "#     def __init__(self, **entries):\n",
    "#         self.__dict__.update(entries)\n",
    "        \n",
    "# args = local_args(**{\n",
    "#     'data_path': '../data_unzip',\n",
    "#     'task': '3E',\n",
    "#     'num_class': 65,\n",
    "#     'class_split': \"0-4\",\n",
    "#     'subset_count': 20,\n",
    "#     'duplicate_time': 0.01,\n",
    "#     'source_lbl_percentage': 0.7,\n",
    "#     'target_lbl_percentage': 0.7,\n",
    "#     'save_path': '..\\train_related\\asd',\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_target_x_filename = '/processed_file_not_one_hot_%s_%1.1f_target_known_label_x.npy'%(args.task, args.target_lbl_percentage)\n",
    "labeled_target_y_filename = '/processed_file_not_one_hot_%s_%1.1f_target_known_label_y.npy'%(args.task, args.target_lbl_percentage)\n",
    "unlabeled_target_x_filename = '/processed_file_not_one_hot_%s_%1.1f_target_unknown_label_x.npy'%(args.task, args.target_lbl_percentage)\n",
    "unlabeled_target_y_filename = '/processed_file_not_one_hot_%s_%1.1f_target_unknown_label_y.npy'%(args.task, args.target_lbl_percentage)\n",
    "labeled_target_x = np.load(args.data_path+labeled_target_x_filename)\n",
    "labeled_target_y = np.load(args.data_path+labeled_target_y_filename)\n",
    "unlabeled_target_x = np.load(args.data_path+unlabeled_target_x_filename)\n",
    "unlabeled_target_y = np.load(args.data_path+unlabeled_target_y_filename)\n",
    "\n",
    "labeled_source_x_filename = '/processed_file_not_one_hot_%s_%1.1f_source_known_label_x.npy'%(args.task, args.source_lbl_percentage)\n",
    "labeled_source_y_filename = '/processed_file_not_one_hot_%s_%1.1f_source_known_label_y.npy'%(args.task, args.source_lbl_percentage)\n",
    "unlabeled_source_x_filename = '/processed_file_not_one_hot_%s_%1.1f_source_unknown_label_x.npy'%(args.task, args.source_lbl_percentage)\n",
    "unlabeled_source_y_filename = '/processed_file_not_one_hot_%s_%1.1f_source_unknown_label_y.npy'%(args.task, args.source_lbl_percentage)\n",
    "labeled_source_x = np.load(args.data_path+labeled_source_x_filename)\n",
    "labeled_source_y = np.load(args.data_path+labeled_source_y_filename)\n",
    "unlabeled_source_x = np.load(args.data_path+unlabeled_source_x_filename)\n",
    "unlabeled_source_y = np.load(args.data_path+unlabeled_source_y_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD for source labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dba_parallel(class_x, verbose=False):\n",
    "    \n",
    "    # randomly chose one from class_x\n",
    "    t_star_ind = np.random.choice(class_x.shape[0], 1)\n",
    "    t_star = class_x[t_star_ind,][0]\n",
    "\n",
    "    # dba\n",
    "    dtw_class_t = np.empty((class_x.shape[0],))\n",
    "    dnn = float('inf')\n",
    "    dnn_ind = float('inf')\n",
    "    for i in tqdm(range(class_x.shape[0])):\n",
    "        dist = dtw(class_x[i], t_star)\n",
    "        dtw_class_t[i] = dist\n",
    "        if dist < dnn and i != t_star_ind:\n",
    "            dnn = dist\n",
    "            dnn_ind = i\n",
    "    weight = np.exp(np.log(0.5) * dtw_class_t / dnn)\n",
    "    dba_avg_t_star = dtw_barycenter_averaging(class_x, weights=weight, max_iter=5, verbose=verbose)\n",
    "    return dba_avg_t_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dba_parallel_warp(class_x, iter_num, core_used = mp.cpu_count() - 2):\n",
    "    # parallel\n",
    "    r = []\n",
    "    for m in range(iter_num // 40 + 1):\n",
    "        print(\"Number of processors used: \", core_used)\n",
    "        start_time = time.time()\n",
    "        with parallel_backend(\"loky\", inner_max_num_threads=core_used):\n",
    "            results = Parallel(n_jobs=core_used)(delayed(dba_parallel)(class_x) for i in range(iter_num))\n",
    "            r.append(results)\n",
    "        end_time = time.time()\n",
    "        print(\"time used: \", end_time - start_time)\n",
    "    return np.array(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### BEGIN ########\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 2\n",
      "Number of processors used:  4\n",
      "error in class 0, skip for now.\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 2\n",
      "Number of processors used:  4\n",
      "time used:  11.95943808555603\n",
      "###### END ########\n",
      "Duriation: 0 hrs; 1 mins; 82 s\n"
     ]
    }
   ],
   "source": [
    "new_data_x = []\n",
    "new_data_y = []\n",
    "\n",
    "start_class, end_class = [int(m) for m in args.class_split.split(\"-\")]\n",
    "\n",
    "mission_left = [0]\n",
    "overall_start = time.time()\n",
    "print(\"###### BEGIN ########\")\n",
    "while len(mission_left) > 0:\n",
    "    print(\"--- trying --- \")\n",
    "    print(\"mission left: \", mission_left)\n",
    "    for i in mission_left:\n",
    "        try:\n",
    "            class_ind = np.where(labeled_source_y == i)\n",
    "            class_ind_subset = class_ind[0][np.random.choice(class_ind[0].shape[0], args.subset_count)]\n",
    "            class_x = labeled_source_x[class_ind_subset]\n",
    "            iter_num_class = round(args.duplicate_time * class_ind[0].shape[0])\n",
    "            print(\"number of data generated for class {}: {}\".format(i, iter_num_class))\n",
    "            results_class = dba_parallel_warp(class_x, iter_num_class) # [iter_num_class, 2]\n",
    "            new_data_x.append(results_class)\n",
    "            new_data_y.extend([np.array([i] * iter_num_class)])\n",
    "            mission_left.pop(mission_left.index(i))\n",
    "            del class_x, class_ind, class_ind_subset, results_class\n",
    "        except:\n",
    "            print(\"error in class {}, skip for now.\".format(i))\n",
    "        \n",
    "new_data_x = np.concatenate(new_data_x, axis=0)\n",
    "new_data_y = np.concatenate(new_data_y, axis=0)\n",
    "overall_end = time.time()\n",
    "duration = timedelta(-1, overall_end - overall_start)\n",
    "print(\"###### END ########\")\n",
    "print(\"Duriation: {} hrs; {} mins; {} s\".format(duration.seconds//3600, duration.seconds//60, duration.seconds))\n",
    "\n",
    "\n",
    "\n",
    "np.save('new_data_x.npy', new_data_x)\n",
    "np.save('new_data_y', new_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
