{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "method used here is based on the following paper\n",
    "\n",
    "Germain Forestier et al. \"Generating synthetic time series to augment sparse datasets\". ICDM 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "sys.path.insert(0, os.path.join(parent_dir,'spring-break'))\n",
    "sys.path.insert(0, os.path.join(parent_dir,'Linear Classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\tslearn\\bases.py:14: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "from FNNLinear import FNNLinear\n",
    "from FNNSeparated import FNNSeparated\n",
    "from GAN import Generator, Discriminator\n",
    "from data_utils import *\n",
    "import argparse\n",
    "import logging\n",
    "import logging.handlers\n",
    "import pickle\n",
    "from centerloss import CenterLoss\n",
    "from DataSetLoader import JoinDataset, SingleDataset\n",
    "from torch.autograd import Variable\n",
    "from binaryloss import BinaryLoss\n",
    "\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier, KNeighborsTimeSeries\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed, parallel_backend, Memory\n",
    "import time\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e8b680dc8fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'JDA Time series adaptation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--data_path\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/projects/rsalakhugroup/complex/domain_adaptation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dataset path\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--task\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"3E\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"task type 3E or 3Av2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--target_lbl_percentage'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'percentage of which target data has label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, default=\"3E\", help=\"task type 3E or 3Av2\")\n",
    "parser.add_argument('--target_lbl_percentage', type=float, default=0.7, help='percentage of which target data has label')\n",
    "parser.add_argument('--source_lbl_percentage', type=float, default=0.7, help='percentage of which source data has label')\n",
    "\n",
    "parser.add_argument(\"--num_class\", type=int, default=\"65\", help=\"num class\")\n",
    "parser.add_argument(\"--class_split\", type=str, default=\"0-4\", help=\"class generated\")\n",
    "parser.add_argument(\"--subset_count\", type=int, default=\"20\", help=\"select number of subset from each class\")\n",
    "parser.add_argument(\"--duplicate_time\", type=int, default=\"1\", help=\"numer of duplication\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"../train_related/asd\", help=\"save path\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local only\n",
    "# class local_args:\n",
    "#     def __init__(self, **entries):\n",
    "#         self.__dict__.update(entries)\n",
    "        \n",
    "# args = local_args(**{\n",
    "#     'data_path': '../data_unzip',\n",
    "#     'task': '3E',\n",
    "#     'num_class': 65,\n",
    "#     'class_split': \"0-4\",\n",
    "#     'subset_count': 20,\n",
    "#     'duplicate_time': 1,\n",
    "#     'source_lbl_percentage': 0.7,\n",
    "#     'target_lbl_percentage': 0.7,\n",
    "#     'save_path': '..\\train_related\\asd',\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_target_x_filename = '/processed_file_not_one_hot_%s_%1.1f_target_known_label_x.npy'%(args.task, args.target_lbl_percentage)\n",
    "labeled_target_y_filename = '/processed_file_not_one_hot_%s_%1.1f_target_known_label_y.npy'%(args.task, args.target_lbl_percentage)\n",
    "unlabeled_target_x_filename = '/processed_file_not_one_hot_%s_%1.1f_target_unknown_label_x.npy'%(args.task, args.target_lbl_percentage)\n",
    "unlabeled_target_y_filename = '/processed_file_not_one_hot_%s_%1.1f_target_unknown_label_y.npy'%(args.task, args.target_lbl_percentage)\n",
    "labeled_target_x = np.load(args.data_path+labeled_target_x_filename)\n",
    "labeled_target_y = np.load(args.data_path+labeled_target_y_filename)\n",
    "unlabeled_target_x = np.load(args.data_path+unlabeled_target_x_filename)\n",
    "unlabeled_target_y = np.load(args.data_path+unlabeled_target_y_filename)\n",
    "\n",
    "labeled_source_x_filename = '/processed_file_not_one_hot_%s_%1.1f_source_known_label_x.npy'%(args.task, args.source_lbl_percentage)\n",
    "labeled_source_y_filename = '/processed_file_not_one_hot_%s_%1.1f_source_known_label_y.npy'%(args.task, args.source_lbl_percentage)\n",
    "unlabeled_source_x_filename = '/processed_file_not_one_hot_%s_%1.1f_source_unknown_label_x.npy'%(args.task, args.source_lbl_percentage)\n",
    "unlabeled_source_y_filename = '/processed_file_not_one_hot_%s_%1.1f_source_unknown_label_y.npy'%(args.task, args.source_lbl_percentage)\n",
    "labeled_source_x = np.load(args.data_path+labeled_source_x_filename)\n",
    "labeled_source_y = np.load(args.data_path+labeled_source_y_filename)\n",
    "unlabeled_source_x = np.load(args.data_path+unlabeled_source_x_filename)\n",
    "unlabeled_source_y = np.load(args.data_path+unlabeled_source_y_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD for source labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dba_parallel(class_x, verbose=False):\n",
    "    \n",
    "    # randomly chose one from class_x\n",
    "    t_star_ind = np.random.choice(class_x.shape[0], 1)\n",
    "    t_star = class_x[t_star_ind,][0]\n",
    "\n",
    "    # dba\n",
    "    dtw_class_t = np.empty((class_x.shape[0],))\n",
    "    dnn = float('inf')\n",
    "    dnn_ind = float('inf')\n",
    "    for i in tqdm(range(class_x.shape[0])):\n",
    "        dist = dtw(class_x[i], t_star)\n",
    "        dtw_class_t[i] = dist\n",
    "        if dist < dnn and i != t_star_ind:\n",
    "            dnn = dist\n",
    "            dnn_ind = i\n",
    "    weight = np.exp(np.log(0.5) * dtw_class_t / dnn)\n",
    "    dba_avg_t_star = dtw_barycenter_averaging(class_x, weights=weight, max_iter=5, verbose=verbose)\n",
    "    return dba_avg_t_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = os.path.join(args.save_path, r'\\cachedir')\n",
    "memory = Memory(cachedir, verbose=0)\n",
    "@memory.cache\n",
    "def dba_parallel_warp(class_x, iter_num, core_used = mp.cpu_count() - 2):\n",
    "    # parallel\n",
    "    print(\"Number of processors used: \", core_used)\n",
    "    start_time = time.time()\n",
    "    with parallel_backend(\"loky\", inner_max_num_threads=core_used):\n",
    "        results = Parallel(n_jobs=core_used)(delayed(dba_parallel)(class_x) for i in range(iter_num))\n",
    "    end_time = time.time()\n",
    "    print(\"time used: \", end_time - start_time)\n",
    "    memory.clear(warn=False)\n",
    "    return np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### BEGIN ########\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 152\n",
      "Number of processors used:  4\n",
      "error in class 0, skip for now.\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 152\n",
      "Number of processors used:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x2162cedaa48 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\parallel.py\", line 340, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\parallel.py\", line 769, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 551, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 160, in submit\n",
      "    fn, *args, **kwargs)\n",
      "  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1027, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in class 0, skip for now.\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 152\n",
      "Number of processors used:  4\n",
      "error in class 0, skip for now.\n",
      "--- trying --- \n",
      "mission left:  [0]\n",
      "number of data generated for class 0: 152\n",
      "Number of processors used:  4\n"
     ]
    }
   ],
   "source": [
    "new_data_x = []\n",
    "new_data_y = []\n",
    "\n",
    "start_class, end_class = [int(m) for m in args.class_split.split(\"-\")]\n",
    "\n",
    "mission_left = [0]\n",
    "overall_start = time.time()\n",
    "print(\"###### BEGIN ########\")\n",
    "while len(mission_left) > 0:\n",
    "    print(\"--- trying --- \")\n",
    "    print(\"mission left: \", mission_left)\n",
    "    for i in mission_left:\n",
    "        try:\n",
    "            class_ind = np.where(labeled_source_y == i)\n",
    "            class_ind_subset = class_ind[0][np.random.choice(class_ind[0].shape[0], args.subset_count)]\n",
    "            class_x = labeled_source_x[class_ind_subset]\n",
    "            iter_num_class = round(args.duplicate_time * class_ind[0].shape[0])\n",
    "            print(\"number of data generated for class {}: {}\".format(i, iter_num_class))\n",
    "            results_class = dba_parallel_warp(class_x, iter_num_class) # [iter_num_class, 2]\n",
    "            new_data_x.append(results_class)\n",
    "            new_data_y.extend([np.array([i] * iter_num_class)])\n",
    "            mission_left.pop(mission_left.index(i))\n",
    "            del class_x, class_ind, class_ind_subset, results_class\n",
    "        except:\n",
    "            print(\"error in class {}, skip for now.\".format(i))\n",
    "        \n",
    "new_data_x = np.concatenate(new_data_x, axis=0)\n",
    "new_data_y = np.concatenate(new_data_y, axis=0)\n",
    "overall_end = time.time()\n",
    "duration = timedelta(-1, overall_end - overall_start)\n",
    "print(\"###### END ########\")\n",
    "print(\"Duriation: {} hrs; {} mins; {} s\".format(duration.seconds//3600, duration.seconds//60, duration.seconds))\n",
    "\n",
    "\n",
    "\n",
    "np.save('new_data_x.npy', new_data_x)\n",
    "np.save('new_data_y', new_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
