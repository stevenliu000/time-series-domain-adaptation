{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset import TimeSeriesDataset, TimeSeriesDatasetConcat\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--task TASK]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--lr_gan LR_GAN] [--lr_clf LR_CLF] [--gap GAP]\n",
      "                             [--lbl_percentage LBL_PERCENTAGE]\n",
      "                             [--num_per_class NUM_PER_CLASS] [--seed SEED]\n",
      "                             [--classifier CLASSIFIER] [--save_path SAVE_PATH]\n",
      "                             [--model_save_period MODEL_SAVE_PERIOD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/stevenliu/Library/Jupyter/runtime/kernel-6a61026b-48c2-4275-9852-19127b43f2be.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenliu/anaconda3/envs/torch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, help='3A or 3E')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='number of epochs')\n",
    "parser.add_argument('--lr_gan', type=float, default=1e-4, help='learning rate for adversarial')\n",
    "parser.add_argument('--lr_clf', type=float, default=1e-4, help='learning rate for classification')\n",
    "parser.add_argument('--gap', type=int, default=4, help='gap: Generator train GAP times, discriminator train once')\n",
    "parser.add_argument('--lbl_percentage', type=float, default=0.2, help='percentage of which target data has label')\n",
    "parser.add_argument('--num_per_class', type=int, default=-1, help='number of sample per class when training local discriminator')\n",
    "parser.add_argument('--seed', type=int, default=0, help='manual seed')\n",
    "parser.add_argument('--classifier', type=str, help='cnet model file')\n",
    "parser.add_argument('--save_path', type=str, default='../train_related/JDA_GAN', help='where to store data')\n",
    "parser.add_argument('--model_save_period', type=int, default=2, help='period in which the model is saved')\n",
    "\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# seed\n",
    "if args.seed is None:\n",
    "    args.seed = random.randint(1, 10000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        \n",
    "args = fake_args(data_path='../data_unzip/', \n",
    "                 task='3A', \n",
    "                 batch_size=100,\n",
    "                 epochs=2,\n",
    "                 lr_gan=1e-3,\n",
    "                 lr_clf=1e-3,\n",
    "                 gap=2,\n",
    "                 lbl_percentage=0.2,\n",
    "                 num_per_class=-1,\n",
    "                 seed=0,\n",
    "                 save_path='../train_related/JDA_GAN',\n",
    "                 model_save_period=1,\n",
    "                 classifier='/Users/stevenliu/time-series-adaption/time-series-domain-adaptation/JDA/FNN_trained_model'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            #nn.Tanh()\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, init_size (small)]\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim, d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h, d_out, dp):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h)\n",
    "        self.fc2 = nn.Linear(d_h, d_out)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_dict(file_path, num_class, data_len=None):\n",
    "    '''\n",
    "    output:\n",
    "        {class: [data]},\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['tr_data']\n",
    "    train_lbl = data_['tr_lbl']\n",
    "    if data_len:\n",
    "        train_data = data_['tr_data'][:data_len]\n",
    "        train_lbl = data_['tr_lbl'][:data_len]\n",
    "    data_dict = get_class_data_dict(train_data, train_lbl, num_class)\n",
    "    \n",
    "    return data_dict, train_data.shape[0]\n",
    "\n",
    "def get_target_dict(file_path, num_class, lbl_percentage, seed=0):\n",
    "    '''\n",
    "    split target domain data\n",
    "    \n",
    "    output:\n",
    "        with label:\n",
    "            {class: [data]}\n",
    "        without label:\n",
    "            [data], [lbl]\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['te_data']\n",
    "    train_lbl = data_['te_lbl']\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[index]\n",
    "    train_lbl = np.argmax(train_lbl[index], -1)\n",
    "\n",
    "    with_label = {i:[] for i in range(num_class)}\n",
    "    labeled_index = []\n",
    "    for i in with_label:\n",
    "        index = np.argwhere(train_lbl==i).flatten()\n",
    "        np.random.seed(seed)\n",
    "        index = np.random.choice(index, int(lbl_percentage*train_lbl.shape[0]/num_class))\n",
    "        labeled_index.append(index)\n",
    "        with_label[i] = train_data[index]\n",
    "\n",
    "    \n",
    "    return with_label, (np.delete(train_data,labeled_index,axis=0), np.delete(train_lbl,labeled_index,axis=0)), train_data.shape[0]\n",
    "\n",
    "def get_class_data_dict(data, lbl, num_class):\n",
    "    '''\n",
    "    construct a dict {label: data}  \n",
    "    '''\n",
    "    lbl_not_one_hot = np.argmax(lbl, -1)\n",
    "    result = {i:[] for i in range(num_class)}\n",
    "    for i in result:\n",
    "        index = np.argwhere(lbl_not_one_hot==i).flatten()\n",
    "        result[i] = data[index]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_batch_source_data_on_class(class_dict, num_per_class):\n",
    "    '''\n",
    "    get batch from source data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for key, value in class_dict.items():\n",
    "        index = random.sample(range(len(value)), num_per_class)\n",
    "        batch_x.extend(value[index])\n",
    "        batch_y.extend([key] * num_per_class)\n",
    "        \n",
    "    return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "def get_batch_target_data_on_class(real_dict, pesudo_dict, unlabel_data, num_per_class, compromise=3, real_weight=1, pesudo_weight=0.01):\n",
    "    '''\n",
    "    get batch from target data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_real_or_pesudo = []\n",
    "    for key in real_dict:\n",
    "        real_num = len(real_dict[key])\n",
    "        pesudo_num = len(pesudo_dict[key])\n",
    "        num_in_class = real_num + pesudo_num\n",
    "        \n",
    "        if num_in_class < num_per_class:\n",
    "            # if totoal number sample in this class is less than the required number of sample\n",
    "            # then fetch the remainding data randomly from the unlabeled set with a compromise\n",
    "            num_fetch_unlabeled = (num_in_class - num_per_class) * compromise\n",
    "            index = random.sample(range(unlabel_data.shape[0]), num_fetch_unlabeled)\n",
    "            batch_x.extend(unlabel_data[index])\n",
    "            batch_y.extend([key] * num_fetch_unlabeled)\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * num_fetch_unlabeled)\n",
    "            \n",
    "            batch_x.extend(real_dict[key])\n",
    "            batch_real_or_pesudo.extend([real_weight] * real_num)\n",
    "            batch_x.extend(pesudo_dict[key])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * pesudo_num)\n",
    "            batch_y.extend([key] * num_in_class)\n",
    "            \n",
    "        else:\n",
    "            index = random.sample(range(num_in_class), num_per_class)\n",
    "            index_in_real = []\n",
    "            index_in_pesudo = []\n",
    "            for i in index:\n",
    "                if i >= real_num:\n",
    "                    index_in_pesudo.append(i-real_num)\n",
    "                else:\n",
    "                    index_in_real.append(i)\n",
    "                    \n",
    "            batch_x.extend(real_dict[key][index_in_real])\n",
    "            batch_real_or_pesudo.extend([real_weight] * len(index_in_real))\n",
    "            batch_x.extend(pesudo_dict[key][index_in_pesudo,:])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * len(index_in_pesudo))\n",
    "            batch_y.extend([key] * num_per_class)\n",
    "    \n",
    "    return np.array(batch_x), np.array(batch_y), np.array(batch_real_or_pesudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL ONYL\n",
    "def classifier(x):\n",
    "    return np.array([1]*x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "\n",
    "encoder: feature extractor\n",
    "CNet:    Classifier\n",
    "DNet_global:    global Discriminator\n",
    "DNet_local:    class-wise Discriminator\n",
    "GNet:    Generator (Adaptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task = '3Av2' if args.task == '3A' else '3E'\n",
    "d_out = 50 if args.task == \"3Av2\" else 65\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if args.num_per_class == -1:\n",
    "    args.num_per_class = math.ceil(args.batch_size / d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict, (target_unlabel_x, target_unlabel_y), target_len  = get_target_dict(args.data_path+'processed_file_%s.pkl'%args.task, d_out, args.lbl_percentage)\n",
    "source_dict, source_len = get_source_dict(args.data_path+'/processed_file_%s.pkl'%args.task, d_out, data_len=target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 1600, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_unlabel_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: restore classifer\n",
    "seq_len = 10 \n",
    "feature_dim = 160\n",
    "classifier_model_folder = 'Final_FNN_' + args.task \n",
    "CNet_path = args.classifier + '/' + classifier_model_folder + \"/CNet_model.ep100\"\n",
    "encoder_path = args.classifier + '/' + classifier_model_folder + \"/Encoder_model.ep100\"\n",
    "    \n",
    "CNet = FNN(d_in=feature_dim * 2 * seq_len, d_h=500, d_out=d_out, dp=0.5)\n",
    "\n",
    "encoder = ComplexTransformer(layers=1,\n",
    "                       time_step=seq_len,\n",
    "                       input_dim=feature_dim,\n",
    "                       hidden_size=512,\n",
    "                       output_dim=512,\n",
    "                       num_heads=8,\n",
    "                       out_dropout=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    CNet.load_state_dict(torch.load(CNet_path))\n",
    "    encoder.load_state_dict(torch.load(encoder_path))\n",
    "else:\n",
    "    CNet.load_state_dict(torch.load(CNet_path, map_location=torch.device('cpu')))\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device('cpu')))\n",
    "\n",
    "def classifier_inference(encoder, CNet, x, x_mean_tr, x_std_tr):\n",
    "    CNet.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        #normalize data\n",
    "        x = (x - x_mean_tr) / x_std_tr\n",
    "        # take the real and imaginary part out\n",
    "        real = x[:,:,0].reshape(args.batch_size, seq_len, feature_dim).float()\n",
    "        imag = x[:,:,1].reshape(args.batch_size, seq_len, feature_dim).float()\n",
    "        if torch.cuda.is_available():\n",
    "            real.to(device)\n",
    "            imag.to(device)\n",
    "        real, imag = encoder(real, imag)\n",
    "        pred = CNet(torch.cat((real, imag), -1).reshape(x.shape[0], -1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:05,  8.43it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total loss: 288.778800, G loss: 190.026870, D_global loss: 64.864285, D_local loss: 33.887646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  8.10it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, total loss: 471.946294, G loss: 424.898793, D_global loss: 26.533259, D_local loss: 20.514242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize GAN\n",
    "real_label = 0.99 # target domain\n",
    "fake_label = 0.01 # source domain\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "DNet_global = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "DNet_local = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "GNet = Generator(feature_dim=feature_dim_joint).to(device)\n",
    "DNet_global.apply(weights_init)\n",
    "DNet_local.apply(weights_init)\n",
    "GNet.apply(weights_init)\n",
    "optimizerD_global = torch.optim.Adam(DNet_global.parameters(), lr=args.lr_gan)\n",
    "optimizerD_local = torch.optim.Adam(DNet_local.parameters(), lr=args.lr_gan)\n",
    "optimizerG = torch.optim.Adam(GNet.parameters(), lr=args.lr_gan)\n",
    "\n",
    "# TODO: add global & local loss\n",
    "criterion_gan_global = nn.BCELoss()\n",
    "criterion_gan_local = nn.BCELoss()\n",
    "schedulerD_global = torch.optim.lr_scheduler.StepLR(optimizerD_global, step_size=30, gamma=0.1)\n",
    "schedulerD_local = torch.optim.lr_scheduler.StepLR(optimizerD_local, step_size=30, gamma=0.1)\n",
    "schedulerG = torch.optim.lr_scheduler.StepLR(optimizerG, step_size=30, gamma=0.1)\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "joint_set = TimeSeriesDatasetConcat(root_dir=args.data_path, file_name='processed_file_%s.pkl'%args.task, seed=args.seed)\n",
    "joint_loader = DataLoader(joint_set, batch_size=args.batch_size, shuffle=True)\n",
    "total_error_D = total_error_G = 0\n",
    "\n",
    "source_mean = joint_set.tr_data_mean\n",
    "target_mean = joint_set.te_data_mean\n",
    "source_std = joint_set.tr_data_std\n",
    "target_std = joint_set.te_data_std\n",
    "\n",
    "D_global_losses = []\n",
    "D_local_losses = []\n",
    "G_losses = []\n",
    "classifier_acc = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    # get pesudo label\n",
    "    correct_target = 0.0\n",
    "    target_pesudo_y = []\n",
    "    for batch in range(math.ceil(target_unlabel_x.shape[0]/args.batch_size)):\n",
    "        target_unlabel_x_batch = torch.Tensor(target_unlabel_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).to(device).float()\n",
    "        target_unlabel_y_batch = torch.Tensor(target_unlabel_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "        pred = classifier_inference(encoder, CNet, target_unlabel_x_batch, target_mean, target_std)\n",
    "        correct_target += (pred.argmax(-1) == target_unlabel_y_batch.argmax(-1)).sum().item()\n",
    "        target_pesudo_y.extend(pred.argmax(-1).numpy())\n",
    "    \n",
    "    target_pesudo_y = np.array(target_pesudo_y)\n",
    "    pesudo_dict = get_class_data_dict(target_unlabel_x, target_pesudo_y, d_out)\n",
    "    print('Epoch: %i, Classifier Acc on Target Domain: %f'%(epoch-1, correct_target/target_unlabel_x.shape[0]))\n",
    "    classifier_acc.append(correct_target/target_unlabel_x.shape[0])\n",
    "    \n",
    "    print('Start Training On global Discriminator')\n",
    "    total_error_D_global = 0\n",
    "    total_error_D_local = 0\n",
    "    total_error_G = 0\n",
    "    total_error_D = total_error_G_global = total_error_G_local = 0\n",
    "    for batch_id, (source_x, target_x) in tqdm(enumerate(joint_loader)):\n",
    "        batch_size = target_x.shape[0]\n",
    "        target_x = target_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        source_x = source_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "\n",
    "        # Data Normalization\n",
    "        target_x = (target_x - target_mean) / target_std\n",
    "        source_x = (source_x - source_mean) / source_std\n",
    "\n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        # train with source domain\n",
    "        DNet_global.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        output = DNet_global(source_data).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_global_source = criterion_gan_global(output, label)\n",
    "        errD_global_source.backward()\n",
    "\n",
    "        # train with target domain\n",
    "        target_data = target_x.to(device).float()\n",
    "        fake = GNet(target_data)\n",
    "        #print(fake)\n",
    "        label.fill_(fake_label)\n",
    "        output = DNet_global(fake.detach()).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_global_target = criterion_gan_global(output, label)\n",
    "        errD_global_target.backward()\n",
    "        total_error_D_global += (errD_global_source + errD_global_target).item()\n",
    "        \n",
    "        if batch_id % args.gap == 0:\n",
    "            optimizerD_global.step()\n",
    "\n",
    "        \"\"\"Update G Network\"\"\"\n",
    "        GNet.zero_grad()\n",
    "        label.fill_(real_label) # fake labels are real for generator cost\n",
    "        output = DNet_global(fake).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        #print()\n",
    "        errG = criterion_gan_global(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        total_error_G += errG.item()\n",
    "\n",
    "    print('Start Training On local Discriminator')\n",
    "    for batch_id in tqdm(range(math.ceil(target_len/args.batch_size))):\n",
    "        get_batch_target_data_on_class(target_dict, pesudo_dict, target_unlabel_x, args.num_per_class)\n",
    "        target_x, target_y, target_weight = get_batch_target_data_on_class(target_dict, pesudo_dict, target_unlabel_x, args.num_per_class)\n",
    "        source_x, source_y = get_batch_source_data_on_class(source_dict, args.num_per_class)\n",
    "        \n",
    "        target_x = torch.Tensor(target_x, device=device)\n",
    "        source_x = torch.Tensor(source_x, device=device)\n",
    "        batch_size = target_x.shape[0]\n",
    "        target_x = target_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        source_x = source_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        \n",
    "        # Data Normalization\n",
    "        target_x = (target_x - target_mean) / target_std\n",
    "        source_x = (source_x - source_mean) / source_std\n",
    "        \n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        # train with source domain\n",
    "        DNet_local.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        output = DNet_local(source_data).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_local_source = criterion_gan_local(output, label)\n",
    "        errD_local_source.backward()\n",
    "\n",
    "        # train with target domain\n",
    "        target_data = target_x.to(device).float()\n",
    "        fake = GNet(target_data)\n",
    "        #print(fake)\n",
    "        label.fill_(fake_label)\n",
    "        output = DNet_local(fake.detach()).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_local_target = criterion_gan_local(output, label)\n",
    "        errD_local_target.backward()\n",
    "        total_error_D_local += (errD_local_source + errD_local_target).item()\n",
    "        \n",
    "        if batch_id % args.gap == 0:\n",
    "            optimizerD_local.step()\n",
    "\n",
    "        \"\"\"Update G Network\"\"\"\n",
    "        GNet.zero_grad()\n",
    "        label.fill_(real_label) # fake labels are real for generator cost\n",
    "        output = DNet_local(fake).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        #print()\n",
    "        errG = criterion_gan_local(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        total_error_G += errG.item()\n",
    "        \n",
    "        \n",
    "    total_error_G = total_error_G/2\n",
    "    schedulerD_global.step()\n",
    "    schedulerD_local.step()\n",
    "    schedulerG.step()\n",
    "    print('Epoch: %i, total loss: %f, G loss: %f, D_global loss: %f, D_local loss: %f'%(\n",
    "        epoch, total_error_D_local+total_error_D_global+total_error_G, total_error_G, total_error_D_global, total_error_D_local))\n",
    "    D_global_losses.append(total_error_D_global)\n",
    "    D_local_losses.append(total_error_D_local)\n",
    "    G_losses.append(total_error_G)\n",
    "    \n",
    "    if epoch % args.model_save_period == 0:\n",
    "        torch.save(DNet_global.state_dict(), args.save_path+'/DNet_global_%i'%epoch)\n",
    "        torch.save(DNet_local.state_dict(), args.save_path+'/DNet_local_%i'%epoch)\n",
    "        torch.save(GNet.state_dict(), args.save_path+'/GNet_%i'%epoch)\n",
    "    \n",
    "    np.save(args.save_path+'/D_global_losses.npy', D_global_losses)\n",
    "    np.save(args.save_path+'/D_local_losses.npy', D_local_losses)\n",
    "    np.save(args.save_path+'/G_loss.npy', G_losses)\n",
    "    np.save(args.save_path+'/classifier_acc.npy', classifier_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
