{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset import TimeSeriesDataset, TimeSeriesDatasetConcat\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--task TASK]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--lr_gan LR_GAN] [--lr_clf LR_CLF] [--gap GAP]\n",
      "                             [--lbl_percentage LBL_PERCENTAGE]\n",
      "                             [--num_per_class NUM_PER_CLASS] [--seed SEED]\n",
      "                             [--classifier CLASSIFIER] [--save_path SAVE_PATH]\n",
      "                             [--model_save_period MODEL_SAVE_PERIOD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/stevenliu/Library/Jupyter/runtime/kernel-6a61026b-48c2-4275-9852-19127b43f2be.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, help='3A or 3E')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='number of epochs')\n",
    "parser.add_argument('--lr_gan', type=float, default=1e-4, help='learning rate for adversarial')\n",
    "parser.add_argument('--lr_clf', type=float, default=1e-4, help='learning rate for classification')\n",
    "parser.add_argument('--gap', type=int, default=4, help='gap: Generator train GAP times, discriminator train once')\n",
    "parser.add_argument('--lbl_percentage', type=float, default=0.2, help='percentage of which target data has label')\n",
    "parser.add_argument('--num_per_class', type=int, default=-1, help='number of sample per class when training local discriminator')\n",
    "parser.add_argument('--seed', type=int, help='manual seed')\n",
    "parser.add_argument('--classifier', type=str, help='cnet model file')\n",
    "parser.add_argument('--save_path', type=str, default='../train_related/JDA_GAN', help='where to store data')\n",
    "parser.add_argument('--model_save_period', type=int, default=2, help='period in which the model is saved')\n",
    "\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# seed\n",
    "if args.seed is None:\n",
    "    args.seed = random.randint(1, 10000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        \n",
    "args = fake_args(data_path='../data_unzip/', \n",
    "                 task='3A', \n",
    "                 batch_size=100,\n",
    "                 epochs=10,\n",
    "                 lr_gan=1e-3,\n",
    "                 lr_clf=1e-3,\n",
    "                 gap=2,\n",
    "                 lbl_percentage=0.2,\n",
    "                 num_per_class=-1,\n",
    "                 seed=0,\n",
    "                 save_path='../train_related/JDA_GAN',\n",
    "                 model_save_period=1,\n",
    "                 classifier='/Users/stevenliu/time-series-adaption/time-series-domain-adaptation/JDA/FNN_trained_model'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            #nn.Tanh()\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, init_size (small)]\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim, d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h, d_out, dp):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h)\n",
    "        self.fc2 = nn.Linear(d_h, d_out)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_dict(file_path, num_class, data_len=None):\n",
    "    '''\n",
    "    output:\n",
    "        {class: [data]},\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['tr_data']\n",
    "    train_lbl = data_['tr_lbl']\n",
    "    if data_len:\n",
    "        train_data = data_['tr_data'][:data_len]\n",
    "        train_lbl = data_['tr_lbl'][:data_len]\n",
    "    data_dict = get_class_data_dict(train_data, train_lbl, num_class)\n",
    "    \n",
    "    return data_dict, train_data.shape[0]\n",
    "\n",
    "def get_target_dict(file_path, num_class, lbl_percentage, seed=0):\n",
    "    '''\n",
    "    split target domain data\n",
    "    \n",
    "    output:\n",
    "        with label:\n",
    "            {class: [data]}\n",
    "        without label:\n",
    "            [data], [lbl]\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['te_data']\n",
    "    train_lbl = data_['te_lbl']\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[index]\n",
    "    train_lbl = np.argmax(train_lbl[index], -1)\n",
    "\n",
    "    with_label = {i:[] for i in range(num_class)}\n",
    "    labeled_index = []\n",
    "    for i in with_label:\n",
    "        index = np.argwhere(train_lbl==i).flatten()\n",
    "        np.random.seed(seed)\n",
    "        index = np.random.choice(index, int(lbl_percentage*train_lbl.shape[0]/num_class))\n",
    "        labeled_index.extend(index)\n",
    "        with_label[i] = train_data[index]\n",
    "        \n",
    "    print(labeled_index)\n",
    "\n",
    "    \n",
    "    return with_label, (np.delete(train_data,labeled_index,axis=0), np.delete(train_lbl,labeled_index,axis=0)), (train_data[labeled_index], train_lbl[labeled_index]), train_data.shape[0]\n",
    "\n",
    "def get_class_data_dict(data, lbl, num_class):\n",
    "    '''\n",
    "    construct a dict {label: data}  \n",
    "    '''\n",
    "    lbl_not_one_hot = np.argmax(lbl, -1)\n",
    "    result = {i:[] for i in range(num_class)}\n",
    "    for i in result:\n",
    "        index = np.argwhere(lbl_not_one_hot==i).flatten()\n",
    "        result[i] = data[index]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_batch_source_data_on_class(class_dict, num_per_class):\n",
    "    '''\n",
    "    get batch from source data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for key, value in class_dict.items():\n",
    "        index = random.sample(range(len(value)), num_per_class)\n",
    "        batch_x.extend(value[index])\n",
    "        batch_y.extend([key] * num_per_class)\n",
    "        \n",
    "    return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "def get_batch_target_data_on_class(real_dict, pesudo_dict, unlabel_data, num_per_class, compromise=3, real_weight=1, pesudo_weight=0.1):\n",
    "    '''\n",
    "    get batch from target data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_real_or_pesudo = []\n",
    "    for key in real_dict:\n",
    "        real_num = len(real_dict[key])\n",
    "        pesudo_num = len(pesudo_dict[key])\n",
    "        num_in_class = real_num + pesudo_num\n",
    "        \n",
    "        if num_in_class < num_per_class:\n",
    "            # if totoal number sample in this class is less than the required number of sample\n",
    "            # then fetch the remainding data randomly from the unlabeled set with a compromise\n",
    "            num_fetch_unlabeled = (num_in_class - num_per_class) * compromise\n",
    "            index = random.sample(range(unlabel_data.shape[0]), num_fetch_unlabeled)\n",
    "            batch_x.extend(unlabel_data[index])\n",
    "            batch_y.extend([key] * num_fetch_unlabeled)\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * num_fetch_unlabeled)\n",
    "            \n",
    "            batch_x.extend(real_dict[key])\n",
    "            batch_real_or_pesudo.extend([real_weight] * real_num)\n",
    "            batch_x.extend(pesudo_dict[key])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * pesudo_num)\n",
    "            batch_y.extend([key] * num_in_class)\n",
    "            \n",
    "        else:\n",
    "            index = random.sample(range(num_in_class), num_per_class)\n",
    "            index_in_real = []\n",
    "            index_in_pesudo = []\n",
    "            for i in index:\n",
    "                if i >= real_num:\n",
    "                    index_in_pesudo.append(i-real_num)\n",
    "                else:\n",
    "                    index_in_real.append(i)\n",
    "                    \n",
    "            batch_x.extend(real_dict[key][index_in_real])\n",
    "            batch_real_or_pesudo.extend([real_weight] * len(index_in_real))\n",
    "            batch_x.extend(pesudo_dict[key][index_in_pesudo,:])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * len(index_in_pesudo))\n",
    "            batch_y.extend([key] * num_per_class)\n",
    "    \n",
    "    return np.array(batch_x), np.array(batch_y), np.array(batch_real_or_pesudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "\n",
    "encoder: feature extractor\n",
    "CNet:    Classifier\n",
    "DNet_global:    global Discriminator\n",
    "DNet_local:    class-wise Discriminator\n",
    "GNet:    Generator (Adaptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.task = '3Av2' if args.task == '3A' else '3E'\n",
    "d_out = 50 if args.task == \"3Av2\" else 65\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if args.num_per_class == -1:\n",
    "    args.num_per_class = math.ceil(args.batch_size / d_out)\n",
    "    \n",
    "model_sub_folder = '/task_%s_gap_%s_lblPer_%i_numPerClass_%i'%(args.task, args.gap, args.lbl_percentage, args.num_per_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, help='3A or 3E')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='number of epochs')\n",
    "parser.add_argument('--lr_gan', type=float, default=1e-4, help='learning rate for adversarial')\n",
    "parser.add_argument('--lr_clf', type=float, default=1e-4, help='learning rate for classification')\n",
    "parser.add_argument('--gap', type=int, default=4, help='gap: Generator train GAP times, discriminator train once')\n",
    "parser.add_argument('--lbl_percentage', type=float, default=0.2, help='percentage of which target data has label')\n",
    "parser.add_argument('--num_per_class', type=int, default=-1, help='number of sample per class when training local discriminator')\n",
    "parser.add_argument('--seed', type=int, help='manual seed')\n",
    "parser.add_argument('--classifier', type=str, help='cnet model file')\n",
    "parser.add_argument('--save_path', type=str, default='../train_related/JDA_GAN', help='where to store data')\n",
    "parser.add_argument('--model_save_period', type=int, default=2, help='period in which the model is saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2388, 2444, 4242, 4266, 4266, 6734, 784, 5232, 1259, 2050, 5395, 4347, 5520, 5520, 913, 3602, 4246, 6696, 2100, 5395, 2407, 3398, 3614, 4587, 4823, 4823, 6710, 457, 5430, 1676, 2960, 5652, 4973, 5750, 5750, 696, 4299, 4634, 6697, 3057, 5652, 3551, 3087, 3112, 4024, 4247, 4247, 6568, 609, 5023, 1467, 2778, 5626, 4396, 5665, 5665, 741, 3712, 4096, 6556, 2835, 5626, 3101, 3113, 3221, 4360, 4468, 4468, 6704, 858, 5648, 2058, 2797, 5746, 4760, 5768, 5768, 1216, 3879, 4415, 6613, 2903, 5746, 3137, 2957, 3115, 4173, 4372, 4372, 6837, 625, 5551, 1323, 2287, 5703, 4749, 5799, 5799, 727, 3976, 4237, 6822, 2506, 5703, 3011, 3425, 4033, 5020, 5163, 5163, 6902, 717, 5779, 1613, 2609, 5871, 5340, 5980, 5980, 827, 4733, 5099, 6899, 3044, 5871, 3797, 2564, 2801, 3921, 4175, 4175, 6857, 603, 5364, 1397, 2225, 5587, 4327, 5624, 5624, 761, 3563, 4007, 6851, 2298, 5587, 2756, 2437, 2640, 4022, 4150, 4150, 6684, 602, 5514, 1338, 2236, 5775, 4561, 5804, 5804, 886, 3512, 4121, 6611, 2293, 5775, 2603, 2716, 2871, 3761, 3839, 3839, 6801, 707, 5342, 1266, 2285, 5687, 4099, 5761, 5761, 749, 3498, 3805, 6657, 2461, 5687, 2840, 2924, 3108, 4194, 4373, 4373, 6636, 659, 5199, 1493, 2155, 5859, 4412, 5887, 5887, 1034, 3749, 4198, 6561, 2373, 5859, 2963, 3222, 3483, 4297, 4575, 4575, 6792, 856, 5360, 1653, 2942, 5622, 4806, 5625, 5625, 1074, 4081, 4309, 6687, 3076, 5622, 3450, 3015, 3174, 4669, 4765, 4765, 6663, 1066, 5369, 1839, 2599, 5513, 4844, 5657, 5657, 1173, 3763, 4693, 6649, 2682, 5513, 3170, 2873, 3381, 4154, 4422, 4422, 6705, 474, 5447, 1156, 2411, 5719, 4747, 5805, 5805, 620, 3750, 4335, 6661, 2711, 5719, 3074, 2426, 2606, 4069, 4413, 4413, 6435, 624, 5194, 1130, 1907, 5444, 4558, 5510, 5510, 653, 3370, 4277, 6331, 2139, 5444, 2490, 3018, 3173, 4093, 4185, 4185, 6806, 436, 5114, 1221, 2314, 5484, 4385, 5672, 5672, 689, 3780, 4160, 6736, 2537, 5484, 3131, 2559, 2717, 3900, 4122, 4122, 6720, 446, 5158, 1220, 2132, 5594, 4168, 5726, 5726, 699, 3575, 3988, 6673, 2232, 5594, 2666, 2513, 2670, 4368, 4789, 4789, 6819, 615, 5908, 1085, 1873, 6084, 5219, 6109, 6109, 668, 3899, 4390, 6782, 1969, 6084, 2620, 3119, 3292, 4005, 4310, 4310, 6948, 923, 5730, 1577, 2485, 6146, 4563, 6174, 6174, 1014, 3806, 4104, 6940, 2685, 6146, 3214, 2948, 3245, 4350, 4589, 4589, 6922, 554, 5868, 1504, 2541, 6064, 4617, 6067, 6067, 694, 3826, 4382, 6905, 2730, 6064, 3091, 3488, 3561, 4567, 4776, 4776, 6706, 826, 5668, 1603, 3231, 6046, 5032, 6071, 6071, 976, 4073, 4713, 6674, 3324, 6046, 3519, 2915, 3079, 4329, 4433, 4433, 6610, 714, 5196, 1414, 2205, 5578, 4481, 5585, 5585, 781, 3939, 4414, 6534, 2652, 5578, 2993, 3117, 3388, 4016, 4152, 4152, 6779, 390, 5287, 1501, 2600, 5677, 4320, 5702, 5702, 601, 3840, 4031, 6774, 2860, 5677, 3321, 2260, 2512, 3711, 4014, 4014, 6520, 308, 5623, 1059, 1571, 5832, 4295, 5876, 5876, 605, 3213, 3765, 6508, 1750, 5832, 2349, 2823, 3004, 3896, 4117, 4117, 6717, 460, 5261, 1135, 2395, 5466, 4430, 5472, 5472, 720, 3558, 3922, 6702, 2601, 5466, 2839, 3372, 3564, 4436, 4572, 4572, 6712, 686, 5301, 1412, 2399, 5572, 4750, 5573, 5573, 884, 4119, 4488, 6541, 2788, 5572, 3448, 2832, 3220, 4377, 4490, 4490, 6846, 481, 5252, 1233, 2123, 5452, 4667, 5545, 5545, 627, 4080, 4447, 6830, 2528, 5452, 3160, 2879, 2974, 4391, 4503, 4503, 6530, 478, 5310, 1225, 2483, 5820, 4576, 5866, 5866, 740, 4116, 4485, 6476, 2562, 5820, 2935, 3223, 3277, 4291, 4406, 4406, 6483, 423, 5248, 1489, 2318, 5505, 4539, 5522, 5522, 920, 3737, 4321, 6440, 2743, 5505, 3255, 2969, 3012, 4196, 4334, 4334, 6540, 690, 5274, 1276, 2202, 5418, 4594, 5474, 5474, 870, 3996, 4268, 6492, 2392, 5418, 2986, 2894, 3242, 4444, 4649, 4649, 6630, 590, 5542, 1406, 2484, 5858, 4818, 5881, 5881, 835, 3967, 4458, 6419, 2612, 5858, 3153, 2896, 3172, 4388, 4545, 4545, 6865, 799, 5411, 1762, 2605, 5607, 4685, 5737, 5737, 998, 3663, 4449, 6852, 2781, 5607, 3096, 2552, 2841, 4325, 4474, 4474, 6692, 369, 5391, 1055, 1931, 5504, 4578, 5595, 5595, 438, 3825, 4330, 6622, 2106, 5504, 2786, 2632, 2929, 3999, 4239, 4239, 6557, 589, 5376, 1129, 2101, 5498, 4470, 5724, 5724, 680, 3672, 4143, 6525, 2372, 5498, 2729, 2748, 3041, 4418, 4541, 4541, 6645, 516, 5592, 1309, 2350, 5904, 4609, 6019, 6019, 695, 3835, 4519, 6639, 2542, 5904, 2965, 3546, 3655, 4923, 4977, 4977, 6833, 648, 5846, 1609, 2764, 6020, 5144, 6079, 6079, 815, 4494, 4933, 6781, 3110, 6020, 3577, 3371, 3427, 4423, 4463, 4463, 6395, 937, 5363, 1566, 3048, 5570, 4555, 5571, 5571, 981, 4105, 4438, 6301, 3144, 5570, 3416, 2751, 2845, 4066, 4241, 4241, 6834, 564, 5759, 1664, 2462, 6156, 4395, 6166, 6166, 875, 3675, 4126, 6763, 2499, 6156, 2843, 3003, 3107, 4301, 4506, 4506, 6451, 750, 5347, 1558, 2563, 5508, 4675, 5518, 5518, 790, 4153, 4305, 6305, 2712, 5508, 3071, 2898, 3106, 4068, 4183, 4183, 6731, 434, 5325, 897, 2636, 5531, 4420, 5532, 5532, 525, 3633, 4112, 6527, 2776, 5531, 3081, 2907, 2936, 3880, 3923, 3923, 6778, 803, 5331, 1516, 2549, 5612, 4006, 5669, 5669, 927, 3447, 3914, 6638, 2708, 5612, 2934, 2878, 2973, 4155, 4357, 4357, 6592, 652, 5354, 1519, 2360, 5604, 4695, 5615, 5615, 772, 3624, 4221, 6587, 2457, 5604, 2961, 2503, 2727, 4020, 4249, 4249, 6693, 463, 5323, 1195, 1976, 5713, 4580, 5728, 5728, 508, 3390, 4026, 6670, 2271, 5713, 2581, 2900, 3010, 4487, 4565, 4565, 6771, 595, 5682, 1704, 2421, 5845, 4715, 5863, 5863, 639, 4079, 4495, 6616, 2575, 5845, 3005, 2595, 2749, 4015, 4232, 4232, 6708, 486, 5183, 1295, 2277, 5423, 4336, 5448, 5448, 953, 3500, 4115, 6590, 2404, 5423, 2665, 3130, 3293, 4278, 4450, 4450, 6827, 805, 5488, 1569, 2436, 5698, 4800, 5786, 5786, 1104, 3991, 4349, 6817, 2774, 5698, 3236, 3049, 3269, 4260, 4405, 4405, 6878, 862, 5568, 1506, 2389, 5783, 4688, 5809, 5809, 993, 3861, 4282, 6868, 2474, 5783, 3169, 3000, 3103, 4089, 4341, 4341, 6621, 568, 5317, 1946, 2624, 5450, 4774, 5461, 5461, 977, 3904, 4222, 6363, 2736, 5450, 3077, 2775, 3078, 4147, 4502, 4502, 6754, 771, 5710, 1509, 2211, 5938, 4607, 6027, 6027, 1002, 3860, 4427, 6660, 2365, 5938, 3035, 3224, 3518, 4448, 4577, 4577, 6805, 675, 5574, 1816, 2763, 5733, 4714, 5781, 5781, 1003, 4279, 4456, 6766, 2818, 5733, 3411, 3083, 3290, 4184, 4355, 4355, 6951, 752, 5253, 1518, 2417, 5506, 4511, 5530, 5530, 836, 3874, 4238, 6907, 2808, 5506, 3176, 2862, 2984, 3972, 4322, 4322, 6410, 778, 4919, 1337, 2371, 5674, 4451, 5798, 5798, 907, 3729, 4082, 6404, 2532, 5674, 2914, 3177, 3325, 4267, 4446, 4446, 6671, 791, 5747, 1451, 2364, 5991, 4650, 6011, 6011, 916, 4074, 4331, 6666, 2555, 5991, 3285, 3157, 3360, 4496, 4703, 4703, 6726, 542, 5771, 1573, 2598, 6021, 4955, 6024, 6024, 990, 3994, 4582, 6668, 2861, 6021, 3259, 2762, 2909, 3864, 4386, 4386, 6617, 810, 5629, 1307, 2180, 5828, 4533, 5830, 5830, 820, 3300, 3947, 6570, 2521, 5828, 2880, 3054, 3180, 4250, 4553, 4553, 6840, 449, 5214, 1431, 2410, 5902, 4741, 5957, 5957, 709, 3848, 4315, 6794, 2705, 5902, 3097, 2306, 2355, 3686, 3902, 3902, 6793, 565, 4998, 1127, 1678, 5421, 4025, 5459, 5459, 768, 3145, 3785, 6777, 1788, 5421, 2351, 2713, 2883, 4044, 4215, 4215, 6652, 550, 5610, 1185, 2186, 5903, 4437, 5933, 5933, 842, 3622, 4172, 6626, 2375, 5903, 2770, 2128, 2336, 3888, 4227, 4227, 6257, 670, 5192, 1341, 1962, 5332, 4440, 5336, 5336, 775, 3394, 4106, 6199, 2023, 5332, 2222, 2798, 2884, 3692, 3887, 3887, 6594, 671, 5396, 1529, 2288, 5608, 4140, 5754, 5754, 885, 3478, 3716, 6548, 2553, 5608, 2863, 3166, 3314, 4092, 4164, 4164, 6753, 906, 5184, 1747, 2619, 5415, 4314, 5490, 5490, 1023, 3799, 4128, 6605, 2728, 5415, 3250, 3038, 3254, 4235, 4586, 4586, 6757, 712, 5655, 1269, 2229, 6007, 4897, 6013, 6013, 1008, 3775, 4367, 6740, 2466, 6007, 3114, 2163, 2464, 3766, 4017, 4017, 6788, 579, 5588, 986, 1919, 5872, 4439, 5878, 5878, 682, 3252, 3808, 6772, 2019, 5872, 2354, 3020, 3364, 4363, 4432, 4432, 6808, 412, 5258, 1342, 2759, 5529, 4544, 5552, 5552, 747, 4045, 4383, 6627, 2858, 5529, 3304, 2807, 2910, 3679, 3831, 3831, 6683, 742, 4701, 1372, 2321, 5138, 4124, 5152, 5152, 1027, 3389, 3734, 6208, 2415, 5138, 2872, 3136, 3369, 4208, 4290, 4290, 6855, 405, 5335, 1172, 2508, 5533, 4434, 5575, 5575, 703, 3859, 4230, 6769, 2709, 5533, 3331]\n"
     ]
    }
   ],
   "source": [
    "target_dict, (target_unlabel_x, target_unlabel_y),(a,b), target_len  = get_target_dict(args.data_path+'processed_file_%s.pkl'%args.task, d_out, args.lbl_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict, (target_unlabel_x, target_unlabel_y),(a,b), target_len  = get_target_dict(args.data_path+'processed_file_%s.pkl'%args.task, d_out, args.lbl_percentage)\n",
    "source_dict, source_len = get_source_dict(args.data_path+'/processed_file_%s.pkl'%args.task, d_out, data_len=target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10 \n",
    "feature_dim = 160\n",
    "classifier_model_folder = 'Final_FNN_' + args.task \n",
    "CNet_path = args.classifier + '/' + classifier_model_folder + \"/CNet_model.ep100\"\n",
    "encoder_path = args.classifier + '/' + classifier_model_folder + \"/Encoder_model.ep100\"\n",
    "    \n",
    "CNet = FNN(d_in=feature_dim * 2 * seq_len, d_h=500, d_out=d_out, dp=0.5)\n",
    "\n",
    "encoder = ComplexTransformer(layers=1,\n",
    "                       time_step=seq_len,\n",
    "                       input_dim=feature_dim,\n",
    "                       hidden_size=512,\n",
    "                       output_dim=512,\n",
    "                       num_heads=8,\n",
    "                       out_dropout=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    CNet.load_state_dict(torch.load(CNet_path))\n",
    "    encoder.load_state_dict(torch.load(encoder_path))\n",
    "else:\n",
    "    CNet.load_state_dict(torch.load(CNet_path, map_location=torch.device('cpu')))\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device('cpu')))\n",
    "\n",
    "def classifier_inference(encoder, CNet, x, x_mean_tr, x_std_tr):\n",
    "    CNet.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        #normalize data\n",
    "        x = (x - x_mean_tr) / x_std_tr\n",
    "        # take the real and imaginary part out\n",
    "        real = x[:,:,0].reshape(args.batch_size, seq_len, feature_dim).float()\n",
    "        imag = x[:,:,1].reshape(args.batch_size, seq_len, feature_dim).float()\n",
    "        if torch.cuda.is_available():\n",
    "            real.to(device)\n",
    "            imag.to(device)\n",
    "        real, imag = encoder(real, imag)\n",
    "        pred = CNet(torch.cat((real, imag), -1).reshape(x.shape[0], -1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  7.92it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total loss: 270.960320, G loss: 164.552835, D_global loss: 64.977936, D_local loss: 41.429550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  7.42it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, total loss: 431.206278, G loss: 352.416420, D_global loss: 37.839625, D_local loss: 40.950233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  7.83it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:06,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, total loss: 371.461570, G loss: 310.787316, D_global loss: 53.721219, D_local loss: 6.953036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  7.83it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:06,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, total loss: 263.580601, G loss: 250.062445, D_global loss: 7.755677, D_local loss: 5.762479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  8.01it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:06,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, total loss: 218.431151, G loss: 204.609731, D_global loss: 7.654703, D_local loss: 6.166717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  7.57it/s]\n",
      "  2%|▏         | 1/50 [00:00<00:06,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training On local Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, total loss: 282.568403, G loss: 94.354383, D_global loss: 134.572747, D_local loss: 53.641273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Classifier Acc on Target Domain: 0.004524\n",
      "Start Training On global Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-29d303fee7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#print(output.mean().item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0merrD_global_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_gan_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0merrD_global_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# train with target domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize GAN\n",
    "real_label = 0.99 # target domain\n",
    "fake_label = 0.01 # source domain\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "DNet_global = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "DNet_local = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "GNet = Generator(feature_dim=feature_dim_joint).to(device)\n",
    "DNet_global.apply(weights_init)\n",
    "DNet_local.apply(weights_init)\n",
    "GNet.apply(weights_init)\n",
    "optimizerD_global = torch.optim.Adam(DNet_global.parameters(), lr=args.lr_gan)\n",
    "optimizerD_local = torch.optim.Adam(DNet_local.parameters(), lr=args.lr_gan)\n",
    "optimizerG = torch.optim.Adam(GNet.parameters(), lr=args.lr_gan)\n",
    "\n",
    "# TODO: add global & local loss\n",
    "criterion_gan_global = nn.BCELoss()\n",
    "criterion_gan_local = nn.BCELoss(reduction='none')\n",
    "schedulerD_global = torch.optim.lr_scheduler.StepLR(optimizerD_global, step_size=30, gamma=0.1)\n",
    "schedulerD_local = torch.optim.lr_scheduler.StepLR(optimizerD_local, step_size=30, gamma=0.1)\n",
    "schedulerG = torch.optim.lr_scheduler.StepLR(optimizerG, step_size=30, gamma=0.1)\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "joint_set = TimeSeriesDatasetConcat(root_dir=args.data_path, file_name='processed_file_%s.pkl'%args.task, seed=args.seed)\n",
    "joint_loader = DataLoader(joint_set, batch_size=args.batch_size, shuffle=True)\n",
    "total_error_D = total_error_G = 0\n",
    "\n",
    "source_mean = joint_set.tr_data_mean\n",
    "target_mean = joint_set.te_data_mean\n",
    "source_std = joint_set.tr_data_std\n",
    "target_std = joint_set.te_data_std\n",
    "\n",
    "D_global_losses = []\n",
    "D_local_losses = []\n",
    "G_losses = []\n",
    "classifier_acc = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    correct_target = 0.0\n",
    "    target_pesudo_y = []\n",
    "    for batch in range(math.ceil(target_unlabel_x.shape[0]/args.batch_size)):\n",
    "        target_unlabel_x_batch = torch.Tensor(target_unlabel_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).to(device).float()\n",
    "        target_unlabel_y_batch = torch.Tensor(target_unlabel_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "        pred = classifier_inference(encoder, CNet, target_unlabel_x_batch, target_mean, target_std)\n",
    "        correct_target += (pred.argmax(-1) == target_unlabel_y_batch.argmax(-1)).sum().item()\n",
    "        target_pesudo_y.extend(pred.argmax(-1).numpy())\n",
    "        \n",
    "    for batch\n",
    "    \n",
    "    target_pesudo_y = np.array(target_pesudo_y)\n",
    "    pesudo_dict = get_class_data_dict(target_unlabel_x, target_pesudo_y, d_out)\n",
    "    print('Epoch: %i, Classifier Acc on Target Domain: %f'%(epoch-1, correct_target/target_unlabel_x.shape[0]))\n",
    "    classifier_acc.append(correct_target/target_unlabel_x.shape[0])\n",
    "    \n",
    "    print('Start Training On global Discriminator')\n",
    "    total_error_D_global = 0\n",
    "    total_error_D_local = 0\n",
    "    total_error_G = 0\n",
    "    total_error_D = total_error_G_global = total_error_G_local = 0\n",
    "    for batch_id, (source_x, target_x) in tqdm(enumerate(joint_loader)):\n",
    "        batch_size = target_x.shape[0]\n",
    "        target_x = target_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        source_x = source_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "\n",
    "        # Data Normalization\n",
    "        target_x = (target_x - target_mean) / target_std\n",
    "        source_x = (source_x - source_mean) / source_std\n",
    "\n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        # train with source domain\n",
    "        DNet_global.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        output = DNet_global(source_data).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_global_source = criterion_gan_global(output, label)\n",
    "        errD_global_source.backward()\n",
    "cun\n",
    "        # train with target domain\n",
    "        target_data = target_x.to(device).float()\n",
    "        fake = GNet(target_data)\n",
    "        #print(fake)\n",
    "        label.fill_(fake_label)\n",
    "        output = DNet_global(fake.detach()).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_global_target = criterion_gan_global(output, label)\n",
    "        errD_global_target.backward()\n",
    "        total_error_D_global += (errD_global_source + errD_global_target).item()\n",
    "        \n",
    "        if batch_id % args.gap == 0:\n",
    "            optimizerD_global.step()\n",
    "\n",
    "        \"\"\"Update G Network\"\"\"\n",
    "        GNet.zero_grad()\n",
    "        label.fill_(real_label) # fake labels are real for generator cost\n",
    "        output = DNet_global(fake).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        #print()\n",
    "        errG = criterion_gan_global(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        total_error_G += errG.item()\n",
    "\n",
    "    print('Start Training On local Discriminator')\n",
    "    for batch_id in tqdm(range(math.ceil(target_len/args.batch_size))):\n",
    "        get_batch_target_data_on_class(target_dict, pesudo_dict, target_unlabel_x, args.num_per_class)\n",
    "        target_x, target_y, target_weight = get_batch_target_data_on_class(target_dict, pesudo_dict, target_unlabel_x, args.num_per_class)\n",
    "        source_x, source_y = get_batch_source_data_on_class(source_dict, args.num_per_class)\n",
    "        \n",
    "        target_x = torch.Tensor(target_x, device=device)\n",
    "        source_x = torch.Tensor(source_x, device=device)\n",
    "        target_weight = torch.Tensor(target_weight, device=device)\n",
    "        batch_size = target_x.shape[0]\n",
    "        target_x = target_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        source_x = source_x.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "        \n",
    "        # Data Normalization\n",
    "        target_x = (target_x - target_mean) / target_std\n",
    "        source_x = (source_x - source_mean) / source_std\n",
    "        \n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        # train with source domain\n",
    "        DNet_local.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        output = DNet_local(source_data).view(-1)\n",
    "        #print(output.mean().item())\n",
    "        errD_local_source = criterion_gan_local(output, label).mean()\n",
    "        errD_local_source.backward()\n",
    "\n",
    "        # train with target domain\n",
    "        target_data = target_x.to(device).float()\n",
    "        fake = GNet(target_data)\n",
    "        #print(fake)\n",
    "        label.fill_(fake_label)\n",
    "        output = DNet_local(fake.detach()).view(-1)\n",
    "        errD_local_target = criterion_gan_local(output, label)\n",
    "        errD_local_target = (errD_local_target * target_weight).mean()\n",
    "        errD_local_target.backward()\n",
    "        total_error_D_local += (errD_local_source + errD_local_target).item()\n",
    "        \n",
    "        if batch_id % args.gap == 0:\n",
    "            optimizerD_local.step()\n",
    "\n",
    "        \"\"\"Update G Network\"\"\"\n",
    "        GNet.zero_grad()\n",
    "        label.fill_(real_label) # fake labels are real for generator cost\n",
    "        output = DNet_local(fake).view(-1)\n",
    "\n",
    "        errG = criterion_gan_local(output, label)\n",
    "        errG = (errG * target_weight).mean()\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        total_error_G += errG.item()\n",
    "        \n",
    "        \n",
    "    total_error_G = total_error_G/2\n",
    "    schedulerD_global.step()\n",
    "    schedulerD_local.step()\n",
    "    schedulerG.step()\n",
    "    print('Epoch: %i, total loss: %f, G loss: %f, D_global loss: %f, D_local loss: %f'%(\n",
    "        epoch, total_error_D_local+total_error_D_global+total_error_G, total_error_G, total_error_D_global, total_error_D_local))\n",
    "    D_global_losses.append(total_error_D_global)\n",
    "    D_local_losses.append(total_error_D_local)\n",
    "    G_losses.append(total_error_G)\n",
    "    \n",
    "    if epoch % args.model_save_period == 0:\n",
    "        torch.save(DNet_global.state_dict(), args.save_path+model_sub_folder+'/DNet_global_%i'%epoch)\n",
    "        torch.save(DNet_local.state_dict(), args.save_path+model_sub_folder+'/DNet_local_%i'%epoch)\n",
    "        torch.save(GNet.state_dict(), args.save_path+model_sub_folder+'/GNet_%i'%epoch)\n",
    "        \n",
    "    np.save(args.save_path+model_sub_folder+'/D_global_losses.npy', D_global_losses)\n",
    "    np.save(args.save_path+model_sub_folder+'/D_local_losses.npy', D_local_losses)\n",
    "    np.save(args.save_path+model_sub_folder+'/G_loss.npy', G_losses)\n",
    "    np.save(args.save_path+model_sub_folder+'/classifier_acc.npy', classifier_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
