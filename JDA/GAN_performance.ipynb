{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset import TimeSeriesDataset, TimeSeriesDatasetConcat\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            #nn.Tanh()\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, init_size (small)]\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim, d_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ) \n",
    "        self.fc = nn.Linear(3200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, seq, feature_dim]\n",
    "        x = self.net(x)\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        out = self.sigmoid(self.fc(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h, d_out, dp):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h)\n",
    "        self.fc2 = nn.Linear(d_h, d_out)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_dict(file_path, num_class, data_len=None):\n",
    "    '''\n",
    "    output:\n",
    "        {class: [data]},\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['tr_data']\n",
    "    train_lbl = data_['tr_lbl']\n",
    "    if data_len:\n",
    "        train_data = data_['tr_data'][:data_len]\n",
    "        train_lbl = data_['tr_lbl'][:data_len]\n",
    "    data_dict = get_class_data_dict(train_data, train_lbl, num_class)\n",
    "    \n",
    "    return data_dict, train_data.shape[0]\n",
    "\n",
    "def get_target_dict(file_path, num_class, lbl_percentage, seed=0):\n",
    "    '''\n",
    "    split target domain data\n",
    "    \n",
    "    output:\n",
    "        with label:\n",
    "            {class: [data]}\n",
    "        without label:\n",
    "            [data], [lbl]\n",
    "        data_len\n",
    "    '''\n",
    "    data_ = np.load(file_path, allow_pickle=True)\n",
    "    train_data = data_['te_data']\n",
    "    train_lbl = data_['te_lbl']\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[index]\n",
    "    train_lbl = np.argmax(train_lbl[index], -1)\n",
    "\n",
    "    with_label = {i:[] for i in range(num_class)}\n",
    "    labeled_index = []\n",
    "    for i in with_label:\n",
    "        index = np.argwhere(train_lbl==i).flatten()\n",
    "        np.random.seed(seed)\n",
    "        index = np.random.choice(index, int(lbl_percentage*train_lbl.shape[0]/num_class))\n",
    "        labeled_index.extend(index)\n",
    "        with_label[i] = train_data[index]\n",
    "\n",
    "    \n",
    "    return with_label, (np.delete(train_data,labeled_index,axis=0), np.delete(train_lbl,labeled_index,axis=0)), (train_data[labeled_index], train_lbl[labeled_index]), train_data.shape[0]\n",
    "\n",
    "def get_class_data_dict(data, lbl, num_class):\n",
    "    '''\n",
    "    construct a dict {label: data}  \n",
    "    '''\n",
    "    lbl_not_one_hot = np.argmax(lbl, -1)\n",
    "    result = {i:[] for i in range(num_class)}\n",
    "    for i in result:\n",
    "        index = np.argwhere(lbl_not_one_hot==i).flatten()\n",
    "        result[i] = data[index]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_batch_source_data_on_class(class_dict, num_per_class):\n",
    "    '''\n",
    "    get batch from source data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for key, value in class_dict.items():\n",
    "        index = random.sample(range(len(value)), num_per_class)\n",
    "        batch_x.extend(value[index])\n",
    "        batch_y.extend([key] * num_per_class)\n",
    "        \n",
    "    return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "def get_batch_target_data_on_class(real_dict, pesudo_dict, unlabel_data, num_per_class, compromise=1, real_weight=1, pesudo_weight=0.5):\n",
    "    '''\n",
    "    get batch from target data given a required number of sample per class\n",
    "    '''\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_real_or_pesudo = []\n",
    "    for key in real_dict:\n",
    "        real_num = len(real_dict[key])\n",
    "        pesudo_num = len(pesudo_dict[key])\n",
    "        num_in_class = real_num + pesudo_num\n",
    "        \n",
    "        if num_in_class < num_per_class:\n",
    "            # if totoal number sample in this class is less than the required number of sample\n",
    "            # then fetch the remainding data randomly from the unlabeled set with a compromise\n",
    "            \n",
    "            num_fetch_unlabeled = (num_per_class - num_in_class) * compromise\n",
    "            index = random.sample(range(unlabel_data.shape[0]), num_fetch_unlabeled)\n",
    "            batch_x.extend(unlabel_data[index])\n",
    "            batch_y.extend([key] * num_fetch_unlabeled)\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * num_fetch_unlabeled)\n",
    "            \n",
    "            batch_x.extend(real_dict[key])\n",
    "            batch_real_or_pesudo.extend([real_weight] * real_num)\n",
    "            batch_x.extend(pesudo_dict[key])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * pesudo_num)\n",
    "            batch_y.extend([key] * num_in_class)\n",
    "            \n",
    "        else:\n",
    "            index = random.sample(range(num_in_class), num_per_class)\n",
    "            index_in_real = []\n",
    "            index_in_pesudo = []\n",
    "            for i in index:\n",
    "                if i >= real_num:\n",
    "                    index_in_pesudo.append(i-real_num)\n",
    "                else:\n",
    "                    index_in_real.append(i)\n",
    "                    \n",
    "            batch_x.extend(real_dict[key][index_in_real])\n",
    "            batch_real_or_pesudo.extend([real_weight] * len(index_in_real))\n",
    "            batch_x.extend(pesudo_dict[key][index_in_pesudo,:])\n",
    "            batch_real_or_pesudo.extend([pesudo_weight] * len(index_in_pesudo))\n",
    "            batch_y.extend([key] * num_per_class)\n",
    "    \n",
    "    return np.array(batch_x), np.array(batch_y), np.array(batch_real_or_pesudo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local only\n",
    "\n",
    "class fake_args():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        \n",
    "# args = fake_args(data_path='../data_unzip/', \n",
    "#                  task='3A', \n",
    "#                  batch_size=100,\n",
    "#                  epochs=10,\n",
    "#                  lr_gan=1e-3,\n",
    "#                  lr_clf=1e-3,\n",
    "#                  gap=2,\n",
    "#                  lbl_percentage=0.2,\n",
    "#                  num_per_class=-1,\n",
    "#                  seed=0,\n",
    "#                  save_path='../train_related/JDA_GAN',\n",
    "#                  model_save_period=1,\n",
    "#                  classifier='/Users/stevenliu/time-series-adaption/time-series-domain-adaptation/JDA/FNN_trained_model'\n",
    "#                  )\n",
    "\n",
    "\n",
    "args = fake_args(data_path='/home/weixinli/',\n",
    "                  task='3E',\n",
    "                  batch_size=100,\n",
    "                  epochs=800,\n",
    "                  gap=4,\n",
    "                  lbl_percentage=1,\n",
    "                  num_per_class=10,\n",
    "                  seed=0,\n",
    "                  classifier='/home/weixinli/time-series-domain-adaptation/JDA/FNN_trained_model')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if args.seed is None:\n",
    "    args.seed = random.randint(1, 10000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "args.task = '3Av2' if args.task == '3A' else '3E'\n",
    "d_out = 50 if args.task == \"3Av2\" else 65\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if args.num_per_class == -1:\n",
    "    args.num_per_class = math.ceil(args.batch_size / d_out)\n",
    "    \n",
    "model_sub_folder = '/task_%s_gap_%s_lblPer_%1.1f_numPerClass_%i'%(args.task, args.gap, args.lbl_percentage, args.num_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Classifier: Unlabel acc: 0.000000, loss: 0.000000; Label acc: 0.016090, loss: 0.101672; Combine acc: 0.015692, loss: 0.101908\n"
     ]
    }
   ],
   "source": [
    "target_dict, (target_unlabel_x, target_unlabel_y), (target_label_x, target_label_y), target_len  = get_target_dict(args.data_path+'processed_file_%s.pkl'%args.task, d_out, args.lbl_percentage)\n",
    "source_dict, source_len = get_source_dict(args.data_path+'/processed_file_%s.pkl'%args.task, d_out, data_len=target_len)\n",
    "\n",
    "seq_len = 10 \n",
    "feature_dim = 160\n",
    "classifier_model_folder = 'Final_FNN_' + args.task \n",
    "CNet_path = args.classifier + '/' + classifier_model_folder + \"/CNet_model.ep100\"\n",
    "encoder_path = args.classifier + '/' + classifier_model_folder + \"/Encoder_model.ep100\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "CNet = FNN(d_in=feature_dim * 2 * seq_len, d_h=500, d_out=d_out, dp=0.5)\n",
    "\n",
    "encoder = ComplexTransformer(layers=1,\n",
    "                       time_step=seq_len,\n",
    "                       input_dim=feature_dim,\n",
    "                       hidden_size=512,\n",
    "                       output_dim=512,\n",
    "                       num_heads=8,\n",
    "                       out_dropout=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    CNet.load_state_dict(torch.load(CNet_path))\n",
    "    encoder.load_state_dict(torch.load(encoder_path))\n",
    "else:\n",
    "    CNet.load_state_dict(torch.load(CNet_path, map_location=torch.device('cpu')))\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device('cpu')))\n",
    "encoder.to(device)\n",
    "CNet.to(device)\n",
    "\n",
    "def classifier_inference(encoder, CNet, x, y, x_mean_tr, x_std_tr, batch_size):\n",
    "    CNet.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        #normalize data\n",
    "        x = (x - x_mean_tr) / x_std_tr\n",
    "        # take the real and imaginary part out\n",
    "        real = x[:,:,0].reshape(batch_size, seq_len, feature_dim).float()\n",
    "        imag = x[:,:,1].reshape(batch_size, seq_len, feature_dim).float()\n",
    "        if torch.cuda.is_available():\n",
    "            real.to(device)\n",
    "            imag.to(device)\n",
    "        real, imag = encoder(real, imag)\n",
    "        pred = CNet(torch.cat((real, imag), -1).reshape(x.shape[0], -1))\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "real_label = 0.99 # target domain\n",
    "fake_label = 0.01 # source domain\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "DNet_global = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "DNet_local = Discriminator(feature_dim=feature_dim_joint, d_out=d_out).to(device)\n",
    "GNet = Generator(feature_dim=feature_dim_joint).to(device)\n",
    "\n",
    "# TODO: add global & local loss\n",
    "criterion_gan_global = nn.BCELoss()\n",
    "criterion_gan_local = nn.BCELoss(reduction='none')\n",
    "\n",
    "feature_dim_joint = 2 * feature_dim\n",
    "joint_set = TimeSeriesDatasetConcat(root_dir=args.data_path, file_name='processed_file_%s.pkl'%args.task, seed=args.seed)\n",
    "joint_loader = DataLoader(joint_set, batch_size=args.batch_size, shuffle=True)\n",
    "total_error_D = total_error_G = 0\n",
    "\n",
    "source_mean = joint_set.tr_data_mean\n",
    "target_mean = joint_set.te_data_mean\n",
    "source_std = joint_set.tr_data_std\n",
    "target_std = joint_set.te_data_std\n",
    "\n",
    "D_global_losses = []\n",
    "D_local_losses = []\n",
    "G_losses = []\n",
    "unlabel_acces = []\n",
    "unlabel_losses = []\n",
    "label_acces = []\n",
    "label_losses = []\n",
    "combine_acces = []\n",
    "combine_losses = []\n",
    "print(device)\n",
    "\n",
    "model_sub_folder = '/task_%s_gap_%s_lblPer_%1.1f_numPerClass_%i/'%(args.task, args.gap, args.lbl_percentage, args.num_per_class)\n",
    "model_Gan_path = '/home/weixinli/time-series-domain-adaptation/train_related/JDA_GAN'\n",
    "GNet_path = model_Gan_path+model_sub_folder+'GNet_795'\n",
    "DNet_global_path=model_Gan_path+model_sub_folder+'DNet_global_795'\n",
    "DNet_local_path= model_Gan_path+model_sub_folder+'DNet_local_795'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GNet.load_state_dict(torch.load(GNet_path))\n",
    "    DNet_global.load_state_dict(torch.load(DNet_global_path))\n",
    "    DNet_local.load_state_dict(torch.load(DNet_local_path))\n",
    "else:\n",
    "    GNet.load_state_dict(torch.load(GNet_path, map_location=torch.device('cpu')))\n",
    "    DNet_global.load_state_dict(torch.load(DNet_global_path, map_location=torch.device('cpu')))\n",
    "    DNet_local.load_state_dict(torch.load(DNet_local_path, map_location=torch.device('cpu')))\n",
    "GNet.to(device)\n",
    "DNet_local.to(device)\n",
    "DNet_global.to(device)\n",
    "\n",
    "unlabel_correct_target = 0.0\n",
    "unlabel_loss = 0.0\n",
    "target_pesudo_y = []\n",
    "\n",
    "for batch in range(math.ceil(target_unlabel_x.shape[0]/args.batch_size)):\n",
    "    batch_size = target_unlabel_x[batch*args.batch_size:(batch+1)*args.batch_size].shape[0]\n",
    "    if batch_size == 0:\n",
    "        continue\n",
    "    target_unlabel_x_batch = torch.tensor(target_unlabel_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).float()\n",
    "    target_unlabel_y_batch = torch.tensor(target_unlabel_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "    target_unlabel_x_batch = target_unlabel_x_batch.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "    target_unlabel_x_batch = (target_unlabel_x_batch - target_mean) / target_std\n",
    "    target_unlabel_x_batch_transform = GNet(target_unlabel_x_batch).reshape(batch_size, -1, 2)\n",
    "    pred, loss = classifier_inference(encoder, CNet, target_unlabel_x_batch_transform, target_unlabel_y_batch, target_mean, target_std, batch_size)\n",
    "    unlabel_correct_target += (pred.argmax(-1) == target_unlabel_y_batch).sum().item()\n",
    "    unlabel_loss += loss.item()\n",
    "    target_pesudo_y.extend(pred.argmax(-1).cpu().numpy())\n",
    "\n",
    "label_correct_target = 0.0\n",
    "label_loss = 0.0\n",
    "for batch in range(math.ceil(target_label_x.shape[0]/args.batch_size)):\n",
    "    batch_size = target_label_x[batch*args.batch_size:(batch+1)*args.batch_size].shape[0]\n",
    "    if batch_size == 0:\n",
    "        continue\n",
    "    target_label_x_batch = torch.tensor(target_label_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).float()\n",
    "    target_label_y_batch = torch.tensor(target_label_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "    target_label_x_batch = target_label_x_batch.reshape(batch_size, seq_len, feature_dim_joint)\n",
    "    target_label_x_batch = (target_label_x_batch - target_mean) / target_std\n",
    "    target_label_x_batch_transform = GNet(target_label_x_batch).reshape(batch_size, -1, 2)\n",
    "    pred, loss = classifier_inference(encoder, CNet, target_label_x_batch_transform, target_label_y_batch, target_mean, target_std, batch_size)\n",
    "    label_correct_target += (pred.argmax(-1) == target_label_y_batch).sum().item()\n",
    "    label_loss += loss.item()\n",
    "    target_pesudo_y.extend(pred.argmax(-1).cpu().numpy()) \n",
    "\n",
    "target_pesudo_y = np.array(target_pesudo_y)\n",
    "pesudo_dict = get_class_data_dict(target_unlabel_x, target_pesudo_y, d_out)\n",
    "if args.lbl_percentage == 1:\n",
    "    unlabel_acc_ = 0\n",
    "    unlabel_loss_ = 0\n",
    "else:\n",
    "    unlabel_acc_ = unlabel_correct_target/target_unlabel_x.shape[0]\n",
    "    unlabel_loss_ = unlabel_loss/target_unlabel_x.shape[0]\n",
    "\n",
    "if args.lbl_percentage == 0: \n",
    "    label_acc_ = 0\n",
    "    label_loss_ = 0\n",
    "else:\n",
    "    label_acc_ = label_correct_target/target_label_x.shape[0]\n",
    "    label_loss_ = label_loss/target_label_x.shape[0]\n",
    "\n",
    "combine_acc_ = (unlabel_correct_target+label_correct_target)/(target_unlabel_x.shape[0]+target_label_x.shape[0])\n",
    "combine_loss_ = (label_loss+unlabel_loss)/(target_unlabel_x.shape[0]+target_label_x.shape[0])\n",
    "print('Classifier: Unlabel acc: %f, loss: %f; Label acc: %f, loss: %f; Combine acc: %f, loss: %f'%(unlabel_acc_,unlabel_loss_,label_acc_,label_loss_,combine_acc_,combine_loss_))\n",
    "\n",
    "unlabel_acces.append(unlabel_acc_)\n",
    "unlabel_losses.append(unlabel_loss_)\n",
    "label_acces.append(label_acc_)\n",
    "label_losses.append(label_loss_)\n",
    "combine_acces.append(combine_acc_)\n",
    "combine_losses.append(combine_loss_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
