{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from martins.complex_transformer import ComplexTransformer\n",
    "from GAN import Generator, Discriminator\n",
    "from data_utils import *\n",
    "import argparse\n",
    "import logging\n",
    "import logging.handlers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinDataset(Dataset):\n",
    "    def __init__(self, root_dir, file_name):\n",
    "        f = open(os.path.join(root_dir, file_name), \"rb\")\n",
    "        dataset = pickle.load(f)\n",
    "        self.source_x = dataset['tr_data']\n",
    "        self.source_y = dataset['tr_lbl']\n",
    "        self.target_x = dataset['te_data']\n",
    "        self.target_y = dataset['te_lbl']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (source_x[index], source_y[index]), (target_x[index], target_y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--task TASK]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--lr_gan LR_GAN] [--lr_clf LR_CLF] [--gap GAP]\n",
      "                             [--lbl_percentage LBL_PERCENTAGE]\n",
      "                             [--num_per_class NUM_PER_CLASS] [--seed SEED]\n",
      "                             [--classifier CLASSIFIER] [--save_path SAVE_PATH]\n",
      "                             [--model_save_period MODEL_SAVE_PERIOD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/stevenliu/Library/Jupyter/runtime/kernel-59462722-2028-457e-8e13-a96282293391.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenliu/anaconda3/envs/torch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='JDA Time series adaptation')\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"/projects/rsalakhugroup/complex/domain_adaptation\", help=\"dataset path\")\n",
    "parser.add_argument(\"--task\", type=str, help='3A or 3E')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='number of epochs')\n",
    "parser.add_argument('--lr_gan', type=float, default=1e-4, help='learning rate for adversarial')\n",
    "parser.add_argument('--lr_clf', type=float, default=1e-4, help='learning rate for classification')\n",
    "parser.add_argument('--gap', type=int, default=4, help='gap: Generator train GAP times, discriminator train once')\n",
    "parser.add_argument('--lbl_percentage', type=float, default=0.2, help='percentage of which target data has label')\n",
    "parser.add_argument('--num_per_class', type=int, default=-1, help='number of sample per class when training local discriminator')\n",
    "parser.add_argument('--seed', type=int, help='manual seed')\n",
    "parser.add_argument('--classifier', type=str, help='cnet model file')\n",
    "parser.add_argument('--save_path', type=str, default='../train_related/JDA_GAN', help='where to store data')\n",
    "parser.add_argument('--model_save_period', type=int, default=2, help='period in which the model is saved')\n",
    "\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# seed\n",
    "if args.seed is None:\n",
    "    args.seed = random.randint(1, 10000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "args.task = '3Av2' if args.task == '3A' else '3E'\n",
    "d_out = 50 if args.task == \"3Av2\" else 65\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if args.num_per_class == -1:\n",
    "    args.num_per_class = math.ceil(args.batch_size / d_out)\n",
    "    \n",
    "model_sub_folder = '/task_%s_gap_%s_lblPer_%i_numPerClass_%i'%(args.task, args.gap, args.lbl_percentage, args.num_per_class)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local only\n",
    "class local_args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "args = local_args(**{\n",
    "    'data_path': '/Users/stevenliu/time-series-adaption/time-series-domain-adaptation/data_unzip',\n",
    "    'task': '3Av2',\n",
    "    'num_class': 50,\n",
    "    'batch_size': 10,\n",
    "    'num_per_class': -1,\n",
    "    'gap': 5,\n",
    "    'lbl_percentage':0.2,\n",
    "    'lr_gan': 1e-4,\n",
    "    'lr_FNN': 1e-4,\n",
    "    'lr_encocer': 1e-4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task = '3Av2' if args.task == '3A' else '3E'\n",
    "d_out = 50 if args.task == \"3Av2\" else 65\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if args.num_per_class == -1:\n",
    "    args.num_per_class = math.ceil(args.batch_size / d_out)\n",
    "    \n",
    "model_sub_folder = '/task_%s_gap_%s_lblPer_%i_numPerClass_%i'%(args.task, args.gap, args.lbl_percentage, args.num_per_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_log_handler = logging.FileHandler('/Users/stevenliu/Downloads/aws/logfile.log')\n",
    "logger.addHandler(file_log_handler)\n",
    "\n",
    "stdout_log_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stdout_log_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict, (target_unlabel_x, target_unlabel_y),(a,b), target_len  = get_target_dict(args.data_path+'/processed_file_%s.pkl'%args.task, d_out, args.lbl_percentage)\n",
    "source_dict, source_len = get_source_dict(args.data_path+'/processed_file_%s.pkl'%args.task, d_out, data_len=target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_dataset = JoinDataset(args.data_path, 'processed_file_%s.pkl'%args.task)\n",
    "join_dataloader = DataLoader(join_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h, d_out, dp):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h)\n",
    "        self.fc2 = nn.Linear(d_h, d_out)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1e2b2ed34ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                              \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                              \u001b[0mout_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                              leaky_slope=0.2)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/complex_transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, time_step, input_dim, hidden_size, output_dim, num_heads, attn_dropout, relu_dropout, res_dropout, out_dropout, attn_mask, reduction_factor, leaky_slope)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Transformer networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/complex_transformer.py\u001b[0m in \u001b[0;36mget_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         return TransformerEncoder(embed_dim=self.embed_dim, num_heads=self.num_heads, layers=self.layers, attn_dropout=self.attn_dropout,\n\u001b[0;32m---> 54\u001b[0;31m             relu_dropout=self.relu_dropout, res_dropout=self.res_dropout, attn_mask=self.attn_mask)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/modules/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, layers, attn_dropout, relu_dropout, res_dropout, attn_mask, leaky_slope)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     leaky_slope=leaky_slope)\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         ])\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/modules/transformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     leaky_slope=leaky_slope)\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         ])\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/modules/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, attn_dropout, relu_dropout, res_dropout, attn_mask, leaky_slope)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0madd_bias_kv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0madd_zero_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/time-series-adaption/time-series-domain-adaptation/martins/modules/multihead_attention.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, attn_dropout, bias, add_bias_kv, add_zero_attn)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "real_label = 0.99 # target domain\n",
    "fake_label = 0.01 # source domain\n",
    "\n",
    "\n",
    "seq_len = 10\n",
    "feature_dim = 160\n",
    "encoder = ComplexTransformer(layers=1,\n",
    "                             time_step=seq_len,\n",
    "                             input_dim=feature_dim,\n",
    "                             hidden_size=64,\n",
    "                             output_dim=64,\n",
    "                             num_heads=8,\n",
    "                             out_dropout=0.5,\n",
    "                             leaky_slope=0.2)\n",
    "encoder.to(device)\n",
    "\n",
    "CNet = FNN(d_in=feature_dim * 2 * seq_len, d_h=500, d_out=d_out, dp=0.7)\n",
    "CNet.to(device)\n",
    "\n",
    "DNet_global = Discriminator(feature_dim=64*20, d_out=d_out).to(device)\n",
    "DNet_local = Discriminator(feature_dim=64*20, d_out=d_out).to(device)\n",
    "GNet = Generator(feature_dim=64*20).to(device)\n",
    "DNet_global.apply(weights_init)\n",
    "DNet_local.apply(weights_init)\n",
    "GNet.apply(weights_init)\n",
    "encoder.apply(weights_init)\n",
    "FNN.apply(weights_init)\n",
    "optimizerD_global = torch.optim.Adam(DNet_global.parameters(), lr=args.lr_gan)\n",
    "optimizerD_local = torch.optim.Adam(DNet_local.parameters(), lr=args.lr_gan)\n",
    "optimizerG = torch.optim.Adam(GNet.parameters(), lr=args.lr_gan)\n",
    "optimizerFNN = torch.optim.Adam(FNN.parameters(), lr=args.lr_FNN)\n",
    "optimizerEncoder = torch.optim.Adam(encoder.parameters(), lr=args.lr_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc_ = []\n",
    "error_D_global = []\n",
    "error_G_global = []\n",
    "error_D_local = []\n",
    "error_G_local = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    # update classifier\n",
    "    # TODO: complete this\n",
    "    for batch_id, (x_batch, y_batch) in tqdm(enumerate()):\n",
    "    \n",
    "    # Assign Pesudo Label\n",
    "    correct_target = 0.0\n",
    "    target_pesudo_y = []\n",
    "    for batch in range(math.ceil(target_unlabel_x.shape[0]/args.batch_size)):\n",
    "        target_unlabel_x_batch = torch.Tensor(target_unlabel_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).to(device).float()\n",
    "        target_unlabel_y_batch = torch.Tensor(target_unlabel_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "        pred = classifier_inference(encoder, CNet, target_unlabel_x_batch, target_mean, target_std)\n",
    "        correct_target += (pred.argmax(-1) == target_unlabel_y_batch.argmax(-1)).sum().item()\n",
    "        target_pesudo_y.extend(pred.argmax(-1).numpy())\n",
    "        \n",
    "    target_pesudo_y = np.array(target_pesudo_y)\n",
    "    pesudo_dict = get_class_data_dict(target_unlabel_x, target_pesudo_y, num_class)\n",
    "        \n",
    "    for batch in range(math.ceil(target_label_x.shape[0]/args.batch_size)):\n",
    "        target_label_x_batch = torch.Tensor(target_label_x[batch*args.batch_size:(batch+1)*args.batch_size], device=device).to(device).float()\n",
    "        target_label_y_batch = torch.Tensor(target_label_y[batch*args.batch_size:(batch+1)*args.batch_size], device=device)\n",
    "        pred = classifier_inference(encoder, CNet, target_label_x_batch, target_mean, target_std)\n",
    "        correct_target += (pred.argmax(-1) == target_label_y_batch.argmax(-1)).sum().item()\n",
    "\n",
    "    logger.info('Epoch: %i, assigned pesudo label with accuracy %f'%(epoch+1, correct_target/(target_unlabel_x.size(0)+target_label_x.size(0)))\n",
    "    target_acc_.append(correct_target/(target_unlabel_x.size(0)+target_label_x.size(0)))\n",
    "                \n",
    "    # Update GAN\n",
    "    # Update global Discriminator\n",
    "    total_error_D_global = 0\n",
    "    total_error_G = 0\n",
    "    for batch_id, ((source_x, source_y), (target_x, target_y)) in tqdm(enumerate(joint_dataloader)):\n",
    "        batch_size = target_x.shape[0]\n",
    "        target_x = target_x.reshape(batch_size, -1)\n",
    "        source_x = source_x.reshape(batch_size, -1)\n",
    "\n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        optimizerD_global.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        source_embedding = encoder(source_data)\n",
    "        target_data = source_y.to(device).float()\n",
    "        target_embedding = encoder(target_data)\n",
    "        fake_source_embedding = GNet(target_embedding).detach()\n",
    "        \n",
    "        # adversarial loss\n",
    "        loss_D_global = DNet_global(fake_source_embedding).mean() - DNet_global(source_embedding).mean()\n",
    "        \n",
    "        total_error_D_global += loss_D_global.item()\n",
    "        \n",
    "        loss_D_global.backward()\n",
    "        optimizerD_global.step()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in DNet_global.parameters():\n",
    "            p.data.clamp_(-args.clip_value, args.clip_value)\n",
    "        \n",
    "        if batch_id % args.n_critic == 0:\n",
    "            \"\"\"Update G Network\"\"\"\n",
    "            optimizerG.zero_grad()\n",
    "            fake_source_embedding = GNet(target_embedding)\n",
    "            \n",
    "            # adversarial loss\n",
    "            loss_G = -DNet_global(fake_source_embedding).mean()\n",
    "            \n",
    "            total_error_G += loss_G.item()\n",
    "            \n",
    "            loss_G.step()\n",
    "            optimizerG.step()\n",
    "            \n",
    "    logger.info('Epoch: %i, Global Discrimator Updates: Loss D_global: %f, Loss G: %f'%(epoch+1, total_error_D_global, total_error_G))\n",
    "    error_D_global.append(total_error_D_global)\n",
    "    error_G_global.append(total_error_G)\n",
    "                \n",
    "    # Update local Discriminator\n",
    "    total_error_D_local = 0\n",
    "    total_error_G = 0\n",
    "    for batch_id in tqdm(range(math.ceil(target_len/args.batch_size))):\n",
    "        target_x, target_y, target_weight = get_batch_target_data_on_class(target_dict, pesudo_dict, target_unlabel_x, args.num_per_class)\n",
    "        source_x, source_y = get_batch_source_data_on_class(source_dict, args.num_per_class)\n",
    "        \n",
    "        source_x = torch.Tensor(source_x, device=device)\n",
    "        target_x = torch.Tensor(target_x, device=device)\n",
    "        source_y = torch.LongTensor(target_y, device=device)\n",
    "        target_y = torch.LongTensor(target_y, device=device)\n",
    "        source_mask = torch.zeros(source_x.size(0), d_out).scatter_(1, source_y.unsqueeze(-1), 1)\n",
    "        target_mask = torch.zeros(target_x.size(0), d_out).scatter_(1, target_y.unsqueeze(-1), 1)\n",
    "        target_weight = torch.zeros(target_x.size(0), d_out).scatter_(1, target_y.unsqueeze(-1), target_weight.unsqueeze(-1))\n",
    "        \n",
    "        torch.Tensor(target_weight, device=device)\n",
    "        batch_size = target_x.shape[0]\n",
    "        \n",
    "        source_x = source_x.reshape(source_x.size(0), -1)\n",
    "        target_x = target_x.reshape(target_x.size(0), -1\n",
    "    \n",
    "        \"\"\"Update D Net\"\"\"\n",
    "        optimizerD_local.zero_grad()\n",
    "        source_data = source_x.to(device).float()\n",
    "        source_embedding = encoder(source_data)\n",
    "        target_data = source_y.to(device).float()\n",
    "        target_embedding = encoder(target_data)\n",
    "        fake_source_embedding = GNet(target_embedding).detach()\n",
    "        \n",
    "        # adversarial loss\n",
    "        source_DNet_local = DNet_local(source_embedding, source_mask)\n",
    "        target_DNet_local = DNet_local(fake_source_embedding, target_mask)\n",
    "        \n",
    "        source_weight_count = source_mask.sum(dim=0)\n",
    "        target_weight_count = target_weight.sum(dim=0)\n",
    "        \n",
    "        source_DNet_local_mean = source_DNet_local.sum(dim=0) / source_weight_count\n",
    "        target_DNet_local_mean = (target_DNet_local * target_weight).sum(dim=0) / target_weight_count        \n",
    "        \n",
    "        loss_D_local = (target_DNet_local_mean - source_DNet_local_mean).sum()\n",
    "        \n",
    "        total_error_D_local += loss_D_local.item()\n",
    "        \n",
    "        loss_D_local.backward()\n",
    "        optimizerD_local.step()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in DNet_local.parameters():\n",
    "            p.data.clamp_(-args.clip_value, args.clip_value)\n",
    "        \n",
    "        if batch_id % args.n_critic == 0:\n",
    "            \"\"\"Update G Network\"\"\"\n",
    "            optimizerG.zero_grad()\n",
    "            fake_source_embedding = GNet(target_embedding)\n",
    "            \n",
    "            # adversarial loss\n",
    "            loss_G = -DNet_local(fake_source_embedding).mean()\n",
    "            \n",
    "            total_error_G += loss_G.item()\n",
    "            \n",
    "            loss_G.step()\n",
    "            optimizerG.step()\n",
    "            \n",
    "    logger.info('Epoch: %i, Local Discrimator Updates: Loss D_global: %f, Loss G: %f'%(epoch+1, total_error_D_local, total_error_G))\n",
    "    error_D_local.append(total_error_D_local)\n",
    "    error_G_global.append(total_error_G)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
